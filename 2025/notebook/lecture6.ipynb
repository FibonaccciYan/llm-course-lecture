{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ea37b0",
   "metadata": {},
   "source": [
    "# Lecture 6 Tutorial: Attention 与 LLaMA 内部解析\n",
    "\n",
    "本 notebook 对应 2025 版第 6 讲的课堂内容，通过 PyTorch 演示注意力模块的关键步骤，帮助在课堂上连贯展示：\n",
    "- 复现标准注意力的逐步计算\n",
    "- 观察 Multi-Head 架构的张量变换\n",
    "- 演示 Grouped Query Attention (GQA) 对 KV 缓存的压缩\n",
    "- 用简化的 Blocked Attention 框架链接理论与实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e61dc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9599786",
   "metadata": {},
   "source": [
    "## 1. 从 hidden states 得到 Q/K/V\n",
    "\n",
    "在 LLaMA 的 decoder layer 中，`q_proj`/`k_proj`/`v_proj` 是线性层，负责把上一层的 hidden states 变换为查询、键和值张量。下面我们用一个可视化规模的示例来模拟这个过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9bd470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states shape: torch.Size([2, 4, 32])\n",
      "q shape: torch.Size([2, 4, 32]), k shape: torch.Size([2, 4, 32]), v shape: torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len = 2, 4\n",
    "num_heads = 4\n",
    "head_dim = 8\n",
    "hidden_size = num_heads * head_dim\n",
    "\n",
    "hidden_states = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "q_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "k_proj_full = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "v_proj_full = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "o_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "q = q_proj(hidden_states)\n",
    "k = k_proj_full(hidden_states)\n",
    "v = v_proj_full(hidden_states)\n",
    "\n",
    "print(f\"hidden_states shape: {hidden_states.shape}\")\n",
    "print(f\"q shape: {q.shape}, k shape: {k.shape}, v shape: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706cb27",
   "metadata": {},
   "source": [
    "## 2. 单头注意力 (Scaled Dot-Product)\n",
    "\n",
    "下面按照讲义的推导，逐步计算 $P$、$\\text{softmax}$ 与输出 $O$。为简洁起见，这里只展示因果 mask（decoder 的自回归约束）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101d24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: torch.Size([2, 4, 4])\n",
      "attention weights sums: tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "context_single shape: torch.Size([2, 4, 32])\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "tensor([[[ 0.1864,    -inf,    -inf,    -inf],\n",
      "         [-0.2687, -0.1780,    -inf,    -inf],\n",
      "         [ 0.2653, -0.3983,  0.2157,    -inf],\n",
      "         [-0.1443, -0.0368,  0.2042,  0.0041]],\n",
      "\n",
      "        [[ 0.0577,    -inf,    -inf,    -inf],\n",
      "         [ 0.1112,  0.0621,    -inf,    -inf],\n",
      "         [ 0.0612,  0.0685, -0.3430,    -inf],\n",
      "         [-0.5373, -0.0201, -0.4858,  0.3036]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scale = 1.0 / math.sqrt(hidden_size)\n",
    "scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
    "scores_masked = scores.masked_fill(causal_mask, float('-inf'))\n",
    "attn_weights = F.softmax(scores_masked, dim=-1)\n",
    "context_single = torch.matmul(attn_weights, v)\n",
    "\n",
    "print('scores shape:', scores.shape)\n",
    "print('attention weights sums:', attn_weights.sum(dim=-1))\n",
    "print('context_single shape:', context_single.shape)\n",
    "print(causal_mask)\n",
    "print(scores_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c86b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1864],\n",
      "         [-0.1780],\n",
      "         [ 0.2653],\n",
      "         [ 0.2042]],\n",
      "\n",
      "        [[ 0.0577],\n",
      "         [ 0.1112],\n",
      "         [ 0.0685],\n",
      "         [ 0.3036]]], grad_fn=<NanToNumBackward0>)\n",
      "manual == torch.softmax? True\n"
     ]
    }
   ],
   "source": [
    "def stable_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    x_max = torch.nan_to_num(x.max(dim=-1, keepdim=True).values)\n",
    "    x_exp = torch.exp(x - x_max)\n",
    "    return x_exp / x_exp.sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "manual_weights = stable_softmax(scores_masked)\n",
    "print('manual == torch.softmax?', torch.allclose(manual_weights, attn_weights, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb1d2b",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention 的张量变换\n",
    "\n",
    "把 $Q/K/V$ 拆成多个头可以让模型在不同的子空间里捕捉特征。注意 reshape、transpose 的顺序与 `Concat` 聚合回原维度的细节。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af16434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_heads shape: torch.Size([2, 4, 4, 8])\n",
      "context_multi shape: torch.Size([2, 4, 32])\n",
      "output_multi shape: torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "q_heads = q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "k_heads = k.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "v_heads = v.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "print('q_heads shape:', q_heads.shape)\n",
    "\n",
    "causal_mask_h = causal_mask.unsqueeze(0).unsqueeze(0)\n",
    "scores_multi = torch.matmul(q_heads, k_heads.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "scores_multi = scores_multi.masked_fill(causal_mask_h, float('-inf'))\n",
    "weights_multi = F.softmax(scores_multi, dim=-1)\n",
    "context_multi = torch.matmul(weights_multi, v_heads)\n",
    "context_multi = context_multi.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "output_multi = o_proj(context_multi)\n",
    "\n",
    "print('context_multi shape:', context_multi.shape)\n",
    "print('output_multi shape:', output_multi.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3dc40",
   "metadata": {},
   "source": [
    "## 4. Grouped Query Attention (GQA)\n",
    "\n",
    "LLaMA-2/3 等模型把 Query 头分成若干组，共享更少数量的 Key/Value 投影，从而减少 KV cache。下面的示例展示了 KV 张量的尺寸如何缩小，并通过 repeat 把它扩展回 Query 头数以完成计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedb2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_heads (full) shape: torch.Size([2, 4, 4, 8])\n",
      "k_kv (GQA cached) shape: torch.Size([2, 2, 4, 8])\n",
      "Full KV cache elements: 256\n",
      "GQA KV cache elements: 128\n",
      "context_gqa shape: torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "num_key_value_heads = 2\n",
    "kv_proj_dim = num_key_value_heads * head_dim\n",
    "\n",
    "k_proj_gqa = nn.Linear(hidden_size, kv_proj_dim, bias=False)\n",
    "v_proj_gqa = nn.Linear(hidden_size, kv_proj_dim, bias=False)\n",
    "\n",
    "k_gqa_raw = k_proj_gqa(hidden_states)\n",
    "v_gqa_raw = v_proj_gqa(hidden_states)\n",
    "\n",
    "k_kv = k_gqa_raw.view(batch_size, seq_len, num_key_value_heads, head_dim).transpose(1, 2).contiguous()\n",
    "v_kv = v_gqa_raw.view(batch_size, seq_len, num_key_value_heads, head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, target_heads: int) -> torch.Tensor:\n",
    "    repeat_factor = target_heads // hidden_states.size(1)\n",
    "    return hidden_states.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "\n",
    "k_gqa_expanded = repeat_kv(k_kv, num_heads)\n",
    "v_gqa_expanded = repeat_kv(v_kv, num_heads)\n",
    "\n",
    "scores_gqa = torch.matmul(q_heads, k_gqa_expanded.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "scores_gqa = scores_gqa.masked_fill(causal_mask_h, float('-inf'))\n",
    "weights_gqa = F.softmax(scores_gqa, dim=-1)\n",
    "context_gqa = torch.matmul(weights_gqa, v_gqa_expanded)\n",
    "context_gqa = context_gqa.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "\n",
    "print('k_heads (full) shape:', k_heads.shape)\n",
    "print('k_kv (GQA cached) shape:', k_kv.shape)\n",
    "print(f'Full KV cache elements: {k_heads.numel():,}')\n",
    "print(f'GQA KV cache elements: {k_kv.numel():,}')\n",
    "print('context_gqa shape:', context_gqa.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c063adb",
   "metadata": {},
   "source": [
    "## 5. Blocked Attention (简化版)\n",
    "\n",
    "BlockedAttention 通过把序列切成块来局部化 QK 计算，从而更好地适配 GPU 并行与缓存。下面用 Python 循环搭建一个教学用的原型，帮助理解讲义里的流程图。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26448d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    dim = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dim)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, v)\n",
    "\n",
    "\n",
    "def blocked_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, block_size: int, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "    bs, seq_len, dim = q.shape\n",
    "    outputs = torch.zeros_like(q)\n",
    "\n",
    "    for start in range(0, seq_len, block_size):\n",
    "        end = min(start + block_size, seq_len)\n",
    "        q_block = q[:, start:end, :]\n",
    "\n",
    "        scores_blocks = []\n",
    "        value_blocks = []\n",
    "        for k_start in range(0, seq_len, block_size):\n",
    "            k_end = min(k_start + block_size, seq_len)\n",
    "            k_block = k[:, k_start:k_end, :]\n",
    "            scores = torch.matmul(q_block, k_block.transpose(-2, -1)) / math.sqrt(dim)\n",
    "            if mask is not None:\n",
    "                mask_block = mask[:, start:end, k_start:k_end]\n",
    "                scores = scores.masked_fill(mask_block, float('-inf'))\n",
    "            scores_blocks.append(scores)\n",
    "            value_blocks.append(v[:, k_start:k_end, :])\n",
    "\n",
    "        scores_concat = torch.cat(scores_blocks, dim=-1)\n",
    "        weights = F.softmax(scores_concat, dim=-1)\n",
    "        values_concat = torch.cat(value_blocks, dim=1)\n",
    "        output_block = torch.matmul(weights, values_concat)\n",
    "        outputs[:, start:end, :] = output_block\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1eff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_attention shape: torch.Size([2, 4, 32])\n",
      "blocked_attention shape: torch.Size([2, 4, 32])\n",
      "max diff: tensor(    0.0000, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "full_attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "blocked_attention_out = blocked_attention(q, k, v, block_size=2, mask=mask)\n",
    "\n",
    "print('full_attention shape:', full_attention.shape)\n",
    "print('blocked_attention shape:', blocked_attention_out.shape)\n",
    "print('max diff:', (full_attention - blocked_attention_out).abs().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634f4b9",
   "metadata": {},
   "source": [
    "## 6. 课堂提示\n",
    "\n",
    "- 把每个代码块与讲义页码对应起来，课堂上可以逐段执行并解释张量尺寸的变化。\n",
    "- 如果要深入 FlashAttention，可以在此基础上展示 CUDA kernel 或调用 `torch.nn.functional.scaled_dot_product_attention` 的对比。\n",
    "- GQA 的 repeat/expand 逻辑与 Hugging Face `LlamaAttention` 中的实现一致，可对照源码进一步解读。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
