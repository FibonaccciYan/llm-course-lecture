{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 笔记导读与目标\n",
    "\n",
    "- 目标: 用可运行示例理解前向/反向、梯度累积与清零、autograd 追踪与分离。\n",
    "- 重点: VJP 视角、`no_grad/torch.inference_mode`、常见 debug 技巧与 A0 作业对应关系。\n",
    "- 建议: 逐格运行，观察 `.grad` 的变化与形状是否匹配预期。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0822df51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.8.0\n",
      "CUDA可用: False\n",
      "MPS可用: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子以便结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS可用: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd 与计算图（动态）\n",
    "\n",
    "- 动态计算图: 前向即时构图，`requires_grad=True` 的张量会被追踪。\n",
    "- 叶子/非叶子: 叶子张量的梯度累计在 `.grad`；中间结果有 `grad_fn`。\n",
    "- 小贴士: 可打印 `tensor.grad_fn`, `tensor.is_leaf`，必要时对中间量调用 `retain_grad()` 便于调试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413501b",
   "metadata": {},
   "source": [
    "## 工程视角：Autograd 你需要知道的（对照讲义）\n",
    "\n",
    "- 动态计算图：前向即时构图；非叶子 `grad_fn`，叶子 `.grad`。\n",
    "- 反向实现（提示）：背后采用反向模式自动微分（VJP），无需显式构造雅可比；详见 PyTorch Autograd 文档。\n",
    "- 非标量 backward：`y.backward(gradient=v)`，`v` 与 `y` 同形，表示上游权重（仅在无显式标量损失时需要）。\n",
    "- 线性层 backward：设上游梯度 `G=∂L/∂Y`，`dX = G @ W.T`，`dW = X.T @ G`，`db = G.sum(批/序列维)`。\n",
    "- 训练循环要点：`zero_grad()`、避免对中间量原地写、参数更新放优化器/`no_grad()`。\n",
    "- 高阶/多次反传：`create_graph=True`、`retain_graph=True` 仅在需要时使用。\n",
    "- 性能/调试：AMP、checkpoint、`torch.compile`、hooks 与图打印。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e245d",
   "metadata": {},
   "source": [
    "### 叶子 vs 非叶子，与 `.grad` / `.grad_fn`\n",
    "\n",
    "- 叶子张量: 用户直接创建且 `requires_grad=True`，无 `grad_fn`，反传后 `.grad` 有值；\n",
    "- 非叶子张量: 由运算产生，默认不保留 `.grad`（需 `retain_grad()` 才能查看），但有 `grad_fn`；\n",
    "- `is_leaf` 可用于快速判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: is_leaf= True , grad_fn= None\n",
      "y: is_leaf= False , grad_fn= <MulBackward0 object at 0x15b8daad0>\n",
      "None\n",
      "x.grad: tensor([2., 2., 2.])\n",
      "y.grad (retained): tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # 叶子\n",
    "y = 2 * x                                             # 非叶子\n",
    "print('x: is_leaf=', x.is_leaf, ', grad_fn=', x.grad_fn)\n",
    "print('y: is_leaf=', y.is_leaf, ', grad_fn=', y.grad_fn)\n",
    "\n",
    "# 默认非叶子不保留 .grad，调用 retain_grad() 以便教学观察\n",
    "y.retain_grad()\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad (retained):', y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53196e1",
   "metadata": {},
   "source": [
    "### 非标量 backward 需要 `gradient` 参数（VJP 视角）\n",
    "\n",
    "- 对非标量张量 `y` 调用 `y.backward()` 会报错；\n",
    "- 需要提供与 `y` 同形的上游向量 `v`：`y.backward(gradient=v)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e19a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "直接 backward 报错: grad can be implicitly created only for scalar outputs\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "提供 gradient 后 x.grad: tensor([0.6667, 1.3333, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2  # 非标量输出 shape=(3,)\n",
    "try:\n",
    "    y.backward()\n",
    "except Exception as e:\n",
    "    print('直接 backward 报错:', e)\n",
    "\n",
    "v = torch.ones_like(y) / y.numel()\n",
    "print(v)\n",
    "y.backward(gradient=v)\n",
    "print('提供 gradient 后 x.grad:', x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f245522",
   "metadata": {},
   "source": [
    "### 高阶梯度与 `create_graph`\n",
    "\n",
    "- 若需对一阶梯度再求梯度，需在第一次求导时设置 `create_graph=True`；\n",
    "- 否则一阶梯度视作常量，无法继续反向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d2528e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1= 27.0  g2= 18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**3\n",
    "# 一阶梯度：dy/dx = 3x^2\n",
    "(g1,) = torch.autograd.grad(y, x, create_graph=True)\n",
    "# 二阶梯度：d^2y/dx^2 = 6x\n",
    "(g2,) = torch.autograd.grad(g1, x)\n",
    "print('g1=', float(g1), ' g2=', float(g2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b894413",
   "metadata": {},
   "source": [
    "### 原地修改与版本计数（常见错误示例）\n",
    "\n",
    "- 对反向所需的张量做原地修改会触发版本计数错误；\n",
    "- 正确做法：参数更新放在优化器或 `with torch.no_grad():` 里，避免修改中间变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee564daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原地修改导致错误: a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2\n",
    "loss = y.sum()\n",
    "# 错误：在建立好计算图后修改叶子 x 会破坏反向所需历史\n",
    "try:\n",
    "    x.add_(1.0)\n",
    "    loss.backward()\n",
    "except Exception as e:\n",
    "    print('原地修改导致错误:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b39ba",
   "metadata": {},
   "source": [
    "### 多次 backward 与 `retain_graph`\n",
    "\n",
    "- 默认反向后会释放计算图；再次反向需 `retain_graph=True` 或重新前向；\n",
    "- 仅当确有需要时才保留计算图（更占内存）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "551152ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次 x.grad: tensor([2., 4., 6.])\n",
      "第二次 x.grad (累加): tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x**2).sum()\n",
    "# 第一次 OK\n",
    "y.backward(retain_graph=True)\n",
    "print('第一次 x.grad:', x.grad)\n",
    "# 第二次如不保留图会报错；上面已设置 retain_graph=True，可再次反向\n",
    "y.backward()\n",
    "print('第二次 x.grad (累加):', x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ed7f2",
   "metadata": {},
   "source": [
    "## 打印/可视化计算图：推荐方案\n",
    "\n",
    "- 小模型用“文本遍历”观察 `grad_fn` 链接关系，轻量无依赖；\n",
    "- 需要图形时优先 `torchviz`（可选依赖），仅用于玩具示例；\n",
    "- 复杂网络更建议用 `torch.fx` 查看前向计算图，配合梯度 hook 观测反向流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23321d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• MeanBackward0\n",
      "  • PowBackward0\n",
      "    • AddmmBackward0\n",
      "      • AccumulateGrad\n",
      "      • ReluBackward0\n",
      "      • TBackward0\n",
      "        • AddmmBackward0\n",
      "        • AccumulateGrad\n",
      "          • AccumulateGrad\n",
      "          • AccumulateGrad\n",
      "          • TBackward0\n",
      "            • AccumulateGrad\n"
     ]
    }
   ],
   "source": [
    "# 轻量 autograd 图遍历（从 loss.grad_fn 出发）\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "def _fn_name(fn):\n",
    "    return fn.__class__.__name__\n",
    "\n",
    "def print_autograd_graph(loss, max_nodes=60):\n",
    "    root = loss.grad_fn\n",
    "    if root is None:\n",
    "        print('loss.grad_fn is None (可能未构图或使用了 no_grad)')\n",
    "        return\n",
    "    q = deque([(root, 0)])\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    while q and count < max_nodes:\n",
    "        fn, d = q.popleft()\n",
    "        if fn in seen:\n",
    "            continue\n",
    "        seen.add(fn)\n",
    "        indent = '  ' * d\n",
    "        print(f\"{indent}• { _fn_name(fn) }\")\n",
    "        count += 1\n",
    "        # next_functions: list[(Function, idx)]\n",
    "        for nxt, _ in getattr(fn, 'next_functions', []):\n",
    "            if nxt is not None and nxt not in seen:\n",
    "                q.append((nxt, d+1))\n",
    "        # 可选：打印保存的张量信息\n",
    "        st = getattr(fn, 'saved_tensors', None)\n",
    "        if st:\n",
    "            for t in st:\n",
    "                try:\n",
    "                    print(f\"{indent}   ⤷ saved tensor: shape={tuple(t.shape)}, dtype={t.dtype}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "# 示例：两层线性网络\n",
    "model = torch.nn.Sequential(torch.nn.Linear(4, 3), torch.nn.ReLU(), torch.nn.Linear(3, 2))\n",
    "x = torch.randn(5, 4, requires_grad=True)\n",
    "y = model(x)\n",
    "loss = (y**2).mean()\n",
    "print_autograd_graph(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45068552",
   "metadata": {},
   "source": [
    "### 可选：用 torchviz 生成图片\n",
    "\n",
    "- 需要安装 `torchviz` 与本地 `graphviz`；环境不具备时可跳过。\n",
    "- 仅在小图上使用，避免渲染巨型网络导致卡顿。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61f972",
   "metadata": {},
   "source": [
    "### torchviz 显示\n",
    "- 下面的单元提供 SVG/PNG 双重回退，并把图保存为文件以便查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c7c87c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"410pt\" height=\"417pt\"\n",
       " viewBox=\"0.00 0.00 410.00 416.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 412.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-412.5 406,-412.5 406,4 -4,4\"/>\n",
       "<!-- 6435695664 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>6435695664</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"233,-31.25 175,-31.25 175,0 233,0 233,-31.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-5.75\" font-family=\"monospace\" font-size=\"10.00\"> (5, 2)</text>\n",
       "</g>\n",
       "<!-- 6435514416 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6435514416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"254,-86.5 154,-86.5 154,-67.25 254,-67.25 254,-86.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 6435514416&#45;&gt;6435695664 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>6435514416&#45;&gt;6435695664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204,-66.88C204,-60.54 204,-51.7 204,-43.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.5,-43.22 204,-33.22 200.5,-43.22 207.5,-43.22\"/>\n",
       "</g>\n",
       "<!-- 6435510864 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6435510864</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139,-141.75 39,-141.75 39,-122.5 139,-122.5 139,-141.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6435510864&#45;&gt;6435514416 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6435510864&#45;&gt;6435514416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M108.51,-122.09C126.52,-113.75 153.52,-101.25 174.23,-91.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.42,-94.97 183.02,-87.59 172.48,-88.61 175.42,-94.97\"/>\n",
       "</g>\n",
       "<!-- 6435554224 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6435554224</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"116,-208.25 62,-208.25 62,-177.75 116,-177.75 116,-208.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-194.75\" font-family=\"monospace\" font-size=\"10.00\">2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-183.5\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 6435554224&#45;&gt;6435510864 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6435554224&#45;&gt;6435510864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M89,-177.45C89,-170.21 89,-161.34 89,-153.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.5,-153.71 89,-143.71 85.5,-153.71 92.5,-153.71\"/>\n",
       "</g>\n",
       "<!-- 6435507888 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6435507888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"251,-141.75 157,-141.75 157,-122.5 251,-122.5 251,-141.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 6435507888&#45;&gt;6435514416 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>6435507888&#45;&gt;6435514416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204,-122.33C204,-115.82 204,-106.67 204,-98.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.5,-98.37 204,-88.37 200.5,-98.37 207.5,-98.37\"/>\n",
       "</g>\n",
       "<!-- 6435517152 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6435517152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"236,-202.62 136,-202.62 136,-183.38 236,-183.38 236,-202.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-189.12\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 6435517152&#45;&gt;6435507888 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6435517152&#45;&gt;6435507888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188.73,-183.06C191.17,-175.1 194.81,-163.19 197.93,-152.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.22,-154.18 200.8,-143.6 194.53,-152.14 201.22,-154.18\"/>\n",
       "</g>\n",
       "<!-- 6435516768 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6435516768</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-269.12 0,-269.12 0,-249.88 100,-249.88 100,-269.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-255.62\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6435516768&#45;&gt;6435517152 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6435516768&#45;&gt;6435517152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.2,-249.4C91.98,-238.59 130.2,-220.46 156.69,-207.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158,-211.15 165.53,-203.71 155,-204.83 158,-211.15\"/>\n",
       "</g>\n",
       "<!-- 6435554128 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6435554128</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-341.62 23,-341.62 23,-311.12 77,-311.12 77,-341.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-328.12\" font-family=\"monospace\" font-size=\"10.00\">0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-316.88\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 6435554128&#45;&gt;6435516768 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6435554128&#45;&gt;6435516768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-310.65C50,-301.82 50,-290.45 50,-280.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-280.89 50,-270.89 46.5,-280.89 53.5,-280.89\"/>\n",
       "</g>\n",
       "<!-- 6435512736 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6435512736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"218,-269.12 118,-269.12 118,-249.88 218,-249.88 218,-269.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-255.62\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6435512736&#45;&gt;6435517152 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6435512736&#45;&gt;6435517152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.5,-249.53C173.08,-240.29 177.19,-225.58 180.52,-213.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.79,-214.92 183.11,-204.35 177.05,-213.04 183.79,-214.92\"/>\n",
       "</g>\n",
       "<!-- 6435695568 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>6435695568</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"197,-342 139,-342 139,-310.75 197,-310.75 197,-342\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\"> (5, 4)</text>\n",
       "</g>\n",
       "<!-- 6435695568&#45;&gt;6435512736 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6435695568&#45;&gt;6435512736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168,-310.33C168,-301.48 168,-290.18 168,-280.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.5,-280.8 168,-270.8 164.5,-280.8 171.5,-280.8\"/>\n",
       "</g>\n",
       "<!-- 6435509040 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>6435509040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"312,-269.12 236,-269.12 236,-249.88 312,-249.88 312,-269.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"274\" y=\"-255.62\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 6435509040&#45;&gt;6435517152 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>6435509040&#45;&gt;6435517152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.76,-249.53C247.79,-239.29 224.66,-222.34 207.63,-209.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"209.77,-207.08 199.64,-203.99 205.63,-212.73 209.77,-207.08\"/>\n",
       "</g>\n",
       "<!-- 6435513792 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>6435513792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"324,-336 224,-336 224,-316.75 324,-316.75 324,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"274\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6435513792&#45;&gt;6435509040 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>6435513792&#45;&gt;6435509040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274,-316.35C274,-307.16 274,-292.58 274,-280.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.5,-280.96 274,-270.96 270.5,-280.96 277.5,-280.96\"/>\n",
       "</g>\n",
       "<!-- 6435554032 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>6435554032</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"306,-408.5 242,-408.5 242,-378 306,-378 306,-408.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"274\" y=\"-395\" font-family=\"monospace\" font-size=\"10.00\">0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"274\" y=\"-383.75\" font-family=\"monospace\" font-size=\"10.00\"> (3, 4)</text>\n",
       "</g>\n",
       "<!-- 6435554032&#45;&gt;6435513792 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>6435554032&#45;&gt;6435513792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274,-377.53C274,-368.69 274,-357.32 274,-347.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.5,-347.76 274,-337.76 270.5,-347.76 277.5,-347.76\"/>\n",
       "</g>\n",
       "<!-- 6435509424 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>6435509424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"381,-141.75 305,-141.75 305,-122.5 381,-122.5 381,-141.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"343\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 6435509424&#45;&gt;6435514416 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>6435509424&#45;&gt;6435514416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M319.42,-122.09C297.16,-113.56 263.51,-100.67 238.28,-91.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.7,-87.8 229.11,-87.49 237.19,-94.34 239.7,-87.8\"/>\n",
       "</g>\n",
       "<!-- 6435512352 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>6435512352</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"402,-202.62 302,-202.62 302,-183.38 402,-183.38 402,-202.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"352\" y=\"-189.12\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6435512352&#45;&gt;6435509424 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>6435512352&#45;&gt;6435509424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M350.63,-183.06C349.43,-175.19 347.64,-163.45 346.09,-153.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.58,-153 344.61,-143.65 342.66,-154.06 349.58,-153\"/>\n",
       "</g>\n",
       "<!-- 6435553840 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>6435553840</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"394,-274.75 330,-274.75 330,-244.25 394,-244.25 394,-274.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-261.25\" font-family=\"monospace\" font-size=\"10.00\">2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (2, 3)</text>\n",
       "</g>\n",
       "<!-- 6435553840&#45;&gt;6435512352 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>6435553840&#45;&gt;6435512352</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.73,-243.86C358.37,-235.08 356.62,-223.77 355.12,-214.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"358.59,-213.66 353.6,-204.32 351.67,-214.74 358.59,-213.66\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x17f90ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存图到: torchviz_graph.svg\n"
     ]
    }
   ],
   "source": [
    "# 尝试用多种方式在 Jupyter 中显示，并保存为文件\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "    from IPython.display import display, SVG, Image\n",
    "    import os\n",
    "    dot = make_dot(y, params=dict(model.named_parameters()))\n",
    "    # 方式1：直接 display 对象（若已配置 _repr_svg_）\n",
    "    try:\n",
    "        display(dot)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # 同步保存为文件\n",
    "    out_base = 'torchviz_graph'\n",
    "    try:\n",
    "        path = dot.render(out_base, format='svg', cleanup=True)\n",
    "        print('已保存图到:', path)\n",
    "    except Exception as e:\n",
    "        print('保存 SVG 失败:', e)\n",
    "        try:\n",
    "            path = dot.render(out_base, format='png', cleanup=True)\n",
    "            print('已保存图到:', path)\n",
    "        except Exception as e2:\n",
    "            print('保存 PNG 失败:', e2)\n",
    "except Exception as e:\n",
    "    print('无法生成/显示图：', e)\n",
    "    print('请确认已安装:pip install torchviz graphviz并在系统层安装 Graphviz 可执行 (dot)。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fc29a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DIM_IN = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "DIM_OUT = 10\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a78dfe",
   "metadata": {},
   "source": [
    "## 实验任务与网络结构\n",
    "\n",
    "- 本笔记用一个两层线性网络演示训练流程与梯度传播。\n",
    "- 观察点: 参数的初值/更新前后变化、`.grad` 在 `backward()` 前后的状态。\n",
    "- 记住: `.grad` 默认累积，优化步前需要清零（见后文）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffaeebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0038,  0.0447, -0.0128, -0.0997,  0.0748,  0.0905, -0.0860,  0.0469,\n",
      "        -0.0531, -0.0035], grad_fn=<SliceBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight[0][0:10]) # just a small slice\n",
    "print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8992c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(152.1913, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "prediction = model(some_input)\n",
    "\n",
    "loss = (ideal_output - prediction).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba805e",
   "metadata": {},
   "source": [
    "## 训练三步与梯度流\n",
    "\n",
    "1. 前向: `prediction = model(x)` 计算损失 `loss`；\n",
    "2. 反向: `loss.backward()` 计算参数与输入的梯度；\n",
    "3. 更新: `optimizer.step()` 使用梯度更新参数；\n",
    "\n",
    "- 清零: 每次迭代前 `optimizer.zero_grad()`，否则 `.grad` 会累加。\n",
    "- 形状: 梯度的形状与对应张量一致，注意广播导致的求和维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "301da334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0038,  0.0447, -0.0128, -0.0997,  0.0748,  0.0905, -0.0860,  0.0469,\n",
      "        -0.0531, -0.0035], grad_fn=<SliceBackward0>)\n",
      "tensor([-1.9702, -7.9875, -3.6117, -1.0854, -3.3561,  4.4059,  2.3645, -1.6472,\n",
      "        -3.5483, -2.0633])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b86eacad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0058,  0.0527, -0.0092, -0.0986,  0.0781,  0.0861, -0.0884,  0.0486,\n",
      "        -0.0495, -0.0015], grad_fn=<SliceBackward0>)\n",
      "tensor([-1.9702, -7.9875, -3.6117, -1.0854, -3.3561,  4.4059,  2.3645, -1.6472,\n",
      "        -3.5483, -2.0633])\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa07c95",
   "metadata": {},
   "source": [
    "## detach / no_grad / inference_mode 区别\n",
    "\n",
    "- `x.detach()`: 切断梯度但共享数据存储，常用于停止梯度或缓存。\n",
    "- `with torch.no_grad()`: 暂停 autograd 记录，常用于推理或 EMA 更新参数；\n",
    "- `with torch.inference_mode()`: 进一步优化推理内存与速度（不可写视角）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f4c624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.weight.grad is None?  True\n"
     ]
    }
   ],
   "source": [
    "# 分离与禁用追踪：与反向传播的关系与应用示例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1) detach 截断梯度\n",
    "encoder = nn.Linear(8, 4)\n",
    "head = nn.Linear(4, 2)\n",
    "x = torch.randn(3, 8)\n",
    "y = torch.randint(0, 2, (3,))\n",
    "\n",
    "h = encoder(x)\n",
    "h_detached = h.detach()       # 不回传到 encoder\n",
    "logits = head(h_detached)\n",
    "loss = nn.CrossEntropyLoss()(logits, y)\n",
    "for p in list(encoder.parameters())+list(head.parameters()):\n",
    "    if p.grad is not None: p.grad.zero_()\n",
    "loss.backward()\n",
    "print('encoder.weight.grad is None? ', encoder.weight.grad is None or torch.all(encoder.weight.grad==0))\n",
    "\n",
    "\n",
    "# 2) 推理/验证不构图\n",
    "model = nn.Linear(8, 2)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(x)\n",
    "\n",
    "# 3) 更高效的纯推理（inference_mode）\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    preds2 = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f8d39f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "527e11c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd417b05",
   "metadata": {},
   "source": [
    "## 函数装饰器方式关闭梯度\n",
    "\n",
    "- `@torch.no_grad()` 等价于在函数体内包裹 `with torch.no_grad():`。\n",
    "- 用于推理接口、评估指标计算，避免无谓的图记录与显存开销。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f641db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dbc32ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::mul        50.79%       1.204ms        50.79%       1.204ms       1.204us          1000  \n",
      "    aten::div        49.21%       1.166ms        49.21%       1.166ms       1.166us          1000  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.370ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    run_on_gpu = True\n",
    "\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z / x) * y\n",
    "\n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
