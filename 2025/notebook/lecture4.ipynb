{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74163b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from torch-test!\n",
      "Excellent! MPS backend is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def main():\n",
    "    print(\"Hello from torch-test!\")\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"Excellent! MPS backend is available.\")\n",
    "    else:\n",
    "        print(\"MPS backend is not available: Something went wrong! Are you running this on a Mac with Apple Silicon chip?\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448d6c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e261693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 59563, 47653, 102667], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "model_name = \"/Users/jingweixu/Downloads/llama3_2_1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# print(tokenizer)\n",
    "input = \"南京大学\"\n",
    "\n",
    "embed_index = tokenizer(input)\n",
    "print(embed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3832d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenizer方法详解 ===\n",
      "原始文本: 南京大学是一所优秀的大学\n",
      "\n",
      "--- 1. tokenize() 方法 ---\n",
      "tokenize() 只做分词，返回字符串token列表，不添加特殊符号\n",
      "tokenize结果: ['åįĹ', 'äº¬', 'å¤§åŃ¦', 'æĺ¯ä¸Ģ', 'æīĢ', 'ä¼ĺç§Ģ', 'çļĦ', 'å¤§åŃ¦']\n",
      "token数量: 8\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer方法详解：encode, decode, tokenize\n",
    "\n",
    "print(\"=== Tokenizer方法详解 ===\")\n",
    "\n",
    "# 使用之前加载的tokenizer\n",
    "# model_name = \"/Users/jingweixu/Downloads/llama3_2_1b\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 测试文本\n",
    "text = \"南京大学是一所优秀的大学\"\n",
    "print(f\"原始文本: {text}\")\n",
    "\n",
    "print(\"\\n--- 1. tokenize() 方法 ---\")\n",
    "print(\"tokenize() 只做分词，返回字符串token列表，不添加特殊符号\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"tokenize结果: {tokens}\")\n",
    "print(f\"token数量: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56cdd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. encode() 方法 ---\n",
      "encode() 将文本转换为token ID，可选择是否添加特殊符号\n",
      "encode(无特殊符号): [59563, 47653, 102667, 107226, 32938, 126047, 9554, 102667]\n",
      "encode(有特殊符号): [128000, 59563, 47653, 102667, 107226, 32938, 126047, 9554, 102667]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 2. encode() 方法 ---\")\n",
    "print(\"encode() 将文本转换为token ID，可选择是否添加特殊符号\")\n",
    "# 不添加特殊符号\n",
    "ids_no_special = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(f\"encode(无特殊符号): {ids_no_special}\")\n",
    "\n",
    "# 添加特殊符号（默认行为）\n",
    "ids_with_special = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(f\"encode(有特殊符号): {ids_with_special}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7005aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. tokenizer() 方法 ---\n",
      "tokenizer() 是完整编码方法，返回字典包含input_ids和attention_mask\n",
      "tokenizer()结果: {'input_ids': [128000, 59563, 47653, 102667, 107226, 32938, 126047, 9554, 102667], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "input_ids: [128000, 59563, 47653, 102667, 107226, 32938, 126047, 9554, 102667]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 3. tokenizer() 方法 ---\")\n",
    "print(\"tokenizer() 是完整编码方法，返回字典包含input_ids和attention_mask\")\n",
    "encoded = tokenizer(text)\n",
    "print(f\"tokenizer()结果: {encoded}\")\n",
    "print(f\"input_ids: {encoded['input_ids']}\")\n",
    "print(f\"attention_mask: {encoded['attention_mask']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58678102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. decode() 方法 ---\n",
      "decode() 将token ID转换回文本\n",
      "解码(无特殊符号): '南京大学是一所优秀的大学'\n",
      "解码(有特殊符号): '<|begin_of_text|>南京大学是一所优秀的大学'\n",
      "解码(完整编码): '<|begin_of_text|>南京大学是一所优秀的大学'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 4. decode() 方法 ---\")\n",
    "print(\"decode() 将token ID转换回文本\")\n",
    "# 解码无特殊符号的ID\n",
    "decoded_no_special = tokenizer.decode(ids_no_special)\n",
    "print(f\"解码(无特殊符号): '{decoded_no_special}'\")\n",
    "\n",
    "# 解码有特殊符号的ID\n",
    "decoded_with_special = tokenizer.decode(ids_with_special)\n",
    "print(f\"解码(有特殊符号): '{decoded_with_special}'\")\n",
    "\n",
    "# 解码完整编码结果\n",
    "decoded_full = tokenizer.decode(encoded['input_ids'])\n",
    "print(f\"解码(完整编码): '{decoded_full}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f591c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. 特殊符号处理 ---\n",
      "查看特殊符号的作用\n",
      "原始文本长度: 12\n",
      "tokenize后token数: 10\n",
      "无特殊符号ID数: 8\n",
      "有特殊符号ID数: 9\n",
      "\n",
      "特殊token信息:\n",
      "BOS token: <|begin_of_text|> (ID: 128000)\n",
      "EOS token: <|eot_id|> (ID: 128009)\n",
      "PAD token: <|eot_id|> (ID: 128009)\n",
      "UNK token: None (ID: None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 5. 特殊符号处理 ---\")\n",
    "print(\"查看特殊符号的作用\")\n",
    "print(f\"原始文本长度: {len(text)}\")\n",
    "print(f\"tokenize后token数: {len(tokens)}\")\n",
    "print(f\"无特殊符号ID数: {len(ids_no_special)}\")\n",
    "print(f\"有特殊符号ID数: {len(ids_with_special)}\")\n",
    "\n",
    "# 查看特殊token\n",
    "print(f\"\\n特殊token信息:\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8adaec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. 批量处理示例 ---\n",
      "批量文本: ['你好', '南京大学', '人工智能']\n",
      "批量编码结果:\n",
      "input_ids形状: torch.Size([3, 4])\n",
      "attention_mask形状: torch.Size([3, 4])\n",
      "input_ids:\n",
      "tensor([[128000,  57668,  53901, 128009],\n",
      "        [128000,  59563,  47653, 102667],\n",
      "        [128000,  17792,  49792, 118034]])\n",
      "批量解码结果: ['你好', '南京大学', '人工智能']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 6. 批量处理示例 ---\")\n",
    "texts = [\"你好\", \"南京大学\", \"人工智能\"]\n",
    "print(f\"批量文本: {texts}\")\n",
    "\n",
    "# 批量编码\n",
    "batch_encoded = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(f\"批量编码结果:\")\n",
    "print(f\"input_ids形状: {batch_encoded['input_ids'].shape}\")\n",
    "print(f\"attention_mask形状: {batch_encoded['attention_mask'].shape}\")\n",
    "print(f\"input_ids:\\n{batch_encoded['input_ids']}\")\n",
    "\n",
    "# 批量解码\n",
    "batch_decoded = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_encoded['input_ids']]\n",
    "print(f\"批量解码结果: {batch_decoded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e838ed",
   "metadata": {},
   "source": [
    "# PyTorch Tensor操作详解\n",
    "\n",
    "本部分将详细讲解PyTorch中常用的tensor操作，包括形状变换、数学运算、索引选择等，并结合LLM中的实际应用场景。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b813081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Broadcasting 基础概念 ===\n",
      "--- 标量与tensor ---\n",
      "a: tensor([1, 2, 3, 4])\n",
      "b: 2\n",
      "a + b: tensor([3, 4, 5, 6])\n",
      "形状: a=torch.Size([4]), b=标量, c=torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 补充：Broadcasting（广播）机制详解\n",
    "\n",
    "print(\"=== Broadcasting 基础概念 ===\")\n",
    "import torch\n",
    "\n",
    "# 1. 标量与tensor的广播\n",
    "print(\"--- 标量与tensor ---\")\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = 2\n",
    "c = a + b\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"a + b: {c}\")\n",
    "print(f\"形状: a={a.shape}, b=标量, c={c.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f963cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 不同形状tensor的广播 ---\n",
      "A形状: torch.Size([3, 4])\n",
      "B形状: torch.Size([4])\n",
      "A: tensor([[ 0.5040,  0.4135,  1.1673, -0.1042],\n",
      "        [-0.3271,  1.1358,  0.3800,  0.4512],\n",
      "        [-1.0532, -1.5775,  0.7869,  1.0367]])\n",
      "B: tensor([-0.2392,  0.6887,  1.0550, -1.1013])\n",
      "A + B形状: torch.Size([3, 4])\n",
      "C: tensor([[ 0.2647,  1.1022,  2.2223, -1.2055],\n",
      "        [-0.5663,  1.8245,  1.4349, -0.6501],\n",
      "        [-1.2924, -0.8888,  1.8419, -0.0646]])\n",
      "广播是否成功: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. 不同形状tensor的广播\n",
    "print(\"\\n--- 不同形状tensor的广播 ---\")\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4)\n",
    "print(f\"A形状: {A.shape}\")\n",
    "print(f\"B形状: {B.shape}\")\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "# 广播过程：B [4] -> [1, 4] -> [3, 4]\n",
    "C = A + B\n",
    "print(f\"A + B形状: {C.shape}\")\n",
    "print(\"C:\", C)\n",
    "print(f\"广播是否成功: {C.shape == (3, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79281a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 使用allclose验证Broadcasting结果一致性 ===\n",
      "--- 1. 标量广播验证 ---\n",
      "tensor([2., 2., 2., 2.])\n",
      "原始tensor: tensor([1., 2., 3., 4.])\n",
      "标量: 2.0\n",
      "Broadcasting结果: tensor([3., 4., 5., 6.])\n",
      "手动编程结果: tensor([3., 4., 5., 6.])\n",
      "结果是否一致: True\n",
      "最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting结果一致性验证\n",
    "\n",
    "print(\"=== 使用allclose验证Broadcasting结果一致性 ===\")\n",
    "\n",
    "# 1. 标量广播验证\n",
    "print(\"--- 1. 标量广播验证 ---\")\n",
    "a = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "b = 2.0\n",
    "\n",
    "# 方法1：使用broadcasting\n",
    "result_broadcast = a + b\n",
    "\n",
    "# 方法2：手动编程\n",
    "result_manual = a + torch.full_like(a, b)\n",
    "\n",
    "print(torch.full_like(a,b))\n",
    "\n",
    "print(f\"原始tensor: {a}\")\n",
    "print(f\"标量: {b}\")\n",
    "print(f\"Broadcasting结果: {result_broadcast}\")\n",
    "print(f\"手动编程结果: {result_manual}\")\n",
    "print(f\"结果是否一致: {torch.allclose(result_broadcast, result_manual)}\")\n",
    "print(f\"最大差异: {torch.max(torch.abs(result_broadcast - result_manual)):.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64c531b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. 向量广播验证 ---\n",
      "B: tensor([-1.1137,  1.0270, -0.8012,  0.7476])\n",
      "B_expanded: tensor([[-1.1137,  1.0270, -0.8012,  0.7476],\n",
      "        [-1.1137,  1.0270, -0.8012,  0.7476],\n",
      "        [-1.1137,  1.0270, -0.8012,  0.7476]])\n",
      "shape of B_expanded: torch.Size([3, 4])\n",
      "矩阵A形状: torch.Size([3, 4])\n",
      "向量B形状: torch.Size([4])\n",
      "Broadcasting结果形状: torch.Size([3, 4])\n",
      "手动编程结果形状: torch.Size([3, 4])\n",
      "结果是否一致: True\n",
      "最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. 向量广播验证\n",
    "print(\"\\n--- 2. 向量广播验证 ---\")\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4)\n",
    "\n",
    "# 方法1：使用broadcasting\n",
    "result_broadcast = A + B\n",
    "\n",
    "# 方法2：手动扩展\n",
    "B_expanded = B.unsqueeze(0).expand(3, 4)\n",
    "\n",
    "print(\"B:\",B)\n",
    "print(\"B_expanded:\", B_expanded)\n",
    "print(\"shape of B_expanded:\", B_expanded.shape)\n",
    "\n",
    "result_manual = A + B_expanded\n",
    "\n",
    "print(f\"矩阵A形状: {A.shape}\")\n",
    "print(f\"向量B形状: {B.shape}\")\n",
    "print(f\"Broadcasting结果形状: {result_broadcast.shape}\")\n",
    "print(f\"手动编程结果形状: {result_manual.shape}\")\n",
    "print(f\"结果是否一致: {torch.allclose(result_broadcast, result_manual)}\")\n",
    "print(f\"最大差异: {torch.max(torch.abs(result_broadcast - result_manual)):.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae94df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. 复杂广播验证 ---\n",
      "B: tensor([[1.7184],\n",
      "        [0.7701],\n",
      "        [0.8592]])\n",
      "B_expanded: tensor([[[1.7184, 1.7184, 1.7184, 1.7184],\n",
      "         [0.7701, 0.7701, 0.7701, 0.7701],\n",
      "         [0.8592, 0.8592, 0.8592, 0.8592]],\n",
      "\n",
      "        [[1.7184, 1.7184, 1.7184, 1.7184],\n",
      "         [0.7701, 0.7701, 0.7701, 0.7701],\n",
      "         [0.8592, 0.8592, 0.8592, 0.8592]]])\n",
      "shape of B_expanded: torch.Size([2, 3, 4])\n",
      "3D tensor A形状: torch.Size([2, 3, 4])\n",
      "2D tensor B形状: torch.Size([3, 1])\n",
      "Broadcasting结果形状: torch.Size([2, 3, 4])\n",
      "手动编程结果形状: torch.Size([2, 3, 4])\n",
      "结果是否一致: True\n",
      "最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. 复杂广播验证\n",
    "print(\"\\n--- 3. 复杂广播验证 ---\")\n",
    "A = torch.randn(2, 3, 4)\n",
    "B = torch.randn(3, 1)\n",
    "\n",
    "# 方法1：使用broadcasting\n",
    "result_broadcast = A + B\n",
    "\n",
    "# 方法2：手动扩展\n",
    "B_expanded = B.unsqueeze(0).expand(2, 3, 4)\n",
    "result_manual = A + B_expanded\n",
    "\n",
    "print(\"B:\",B)\n",
    "print(\"B_expanded:\", B_expanded)\n",
    "print(\"shape of B_expanded:\", B_expanded.shape)\n",
    "\n",
    "\n",
    "print(f\"3D tensor A形状: {A.shape}\")\n",
    "print(f\"2D tensor B形状: {B.shape}\")\n",
    "print(f\"Broadcasting结果形状: {result_broadcast.shape}\")\n",
    "print(f\"手动编程结果形状: {result_manual.shape}\")\n",
    "print(f\"结果是否一致: {torch.allclose(result_broadcast, result_manual)}\")\n",
    "print(f\"最大差异: {torch.max(torch.abs(result_broadcast - result_manual)):.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ac9e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. 矩阵乘法中的广播验证 ---\n",
      "A: tensor([[ 0.3280,  1.1160,  0.6442,  1.5937],\n",
      "        [-0.2867,  1.2730,  0.4498,  1.7053],\n",
      "        [-0.9767,  0.2935,  0.9173, -0.4949]])\n",
      "A_expanded: tensor([[[ 0.3280,  1.1160,  0.6442,  1.5937],\n",
      "         [-0.2867,  1.2730,  0.4498,  1.7053],\n",
      "         [-0.9767,  0.2935,  0.9173, -0.4949]],\n",
      "\n",
      "        [[ 0.3280,  1.1160,  0.6442,  1.5937],\n",
      "         [-0.2867,  1.2730,  0.4498,  1.7053],\n",
      "         [-0.9767,  0.2935,  0.9173, -0.4949]]])\n",
      "shape of A_expanded: torch.Size([2, 3, 4])\n",
      "矩阵A形状: torch.Size([3, 4])\n",
      "批量矩阵B形状: torch.Size([2, 4, 5])\n",
      "Broadcasting matmul结果形状: torch.Size([2, 3, 5])\n",
      "手动bmm结果形状: torch.Size([2, 3, 5])\n",
      "结果是否一致: True\n",
      "最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. 矩阵乘法中的广播验证\n",
    "print(\"\\n--- 4. 矩阵乘法中的广播验证 ---\")\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(2, 4, 5)\n",
    "\n",
    "# 方法1：使用matmul的广播\n",
    "result_broadcast = torch.matmul(A, B)\n",
    "\n",
    "# 方法2：手动扩展A然后计算\n",
    "A_expanded = A.unsqueeze(0).expand(2, 3, 4)\n",
    "result_manual = torch.bmm(A_expanded, B)\n",
    "\n",
    "\n",
    "print(\"A:\",A)\n",
    "print(\"A_expanded:\", A_expanded)\n",
    "print(\"shape of A_expanded:\", A_expanded.shape)\n",
    "\n",
    "print(f\"矩阵A形状: {A.shape}\")\n",
    "print(f\"批量矩阵B形状: {B.shape}\")\n",
    "print(f\"Broadcasting matmul结果形状: {result_broadcast.shape}\")\n",
    "print(f\"手动bmm结果形状: {result_manual.shape}\")\n",
    "print(f\"结果是否一致: {torch.allclose(result_broadcast, result_manual)}\")\n",
    "print(f\"最大差异: {torch.max(torch.abs(result_broadcast - result_manual)):.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6ac5432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. 批量归一化中的广播验证 ---\n",
      "输入形状: torch.Size([32, 64, 28, 28])\n",
      "Broadcasting方法结果形状: torch.Size([32, 64, 28, 28])\n",
      "手动扩展方法结果形状: torch.Size([32, 64, 28, 28])\n",
      "结果是否一致: True\n",
      "最大差异: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 批量归一化中的广播验证\n",
    "print(\"\\n--- 5. 批量归一化中的广播验证 ---\")\n",
    "x = torch.randn(32, 64, 28, 28)\n",
    "\n",
    "# 方法1：使用keepdim=True的广播\n",
    "mean_broadcast = torch.mean(x, dim=(0, 2, 3), keepdim=True)\n",
    "std_broadcast = torch.std(x, dim=(0, 2, 3), keepdim=True)\n",
    "normalized_broadcast = (x - mean_broadcast) / (std_broadcast + 1e-8)\n",
    "\n",
    "# 方法2：手动扩展\n",
    "mean_manual = torch.mean(x, dim=(0, 2, 3))  # [64]\n",
    "std_manual = torch.std(x, dim=(0, 2, 3))    # [64]\n",
    "mean_expanded = mean_manual.view(1, 64, 1, 1).expand(32, 64, 28, 28)\n",
    "std_expanded = std_manual.view(1, 64, 1, 1).expand(32, 64, 28, 28)\n",
    "normalized_manual = (x - mean_expanded) / (std_expanded + 1e-8)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"Broadcasting方法结果形状: {normalized_broadcast.shape}\")\n",
    "print(f\"手动扩展方法结果形状: {normalized_manual.shape}\")\n",
    "print(f\"结果是否一致: {torch.allclose(normalized_broadcast, normalized_manual)}\")\n",
    "print(f\"最大差异: {torch.max(torch.abs(normalized_broadcast - normalized_manual)):.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e320a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. 注意力机制中的广播验证 ---\n",
      "注意力分数形状: torch.Size([2, 8, 8])\n",
      "mask形状: torch.Size([8])\n",
      "Broadcasting方法结果形状: torch.Size([2, 8, 8])\n",
      "手动扩展方法结果形状: torch.Size([2, 8, 8])\n",
      "结果是否一致: True\n",
      "最大差异: 0.0000000000\n",
      "\n",
      "=== 总结 ===\n",
      "✓ 所有broadcasting运算结果都与手动编程结果完全一致\n",
      "✓ 最大差异都在机器精度范围内（~1e-10）\n",
      "✓ 这证明了broadcasting机制的正确性和可靠性\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. 注意力机制中的广播验证\n",
    "print(\"\\n--- 6. 注意力机制中的广播验证 ---\")\n",
    "scores = torch.randn(2, 8, 8)\n",
    "mask = torch.tensor([1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# 方法1：使用unsqueeze的广播\n",
    "masked_scores_broadcast = scores + mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 方法2：手动扩展\n",
    "mask_expanded = mask.view(1, 1, 8).expand(2, 8, 8)\n",
    "masked_scores_manual = scores + mask_expanded\n",
    "\n",
    "print(f\"注意力分数形状: {scores.shape}\")\n",
    "print(f\"mask形状: {mask.shape}\")\n",
    "print(f\"Broadcasting方法结果形状: {masked_scores_broadcast.shape}\")\n",
    "print(f\"手动扩展方法结果形状: {masked_scores_manual.shape}\")\n",
    "print(f\"结果是否一致: {torch.allclose(masked_scores_broadcast, masked_scores_manual)}\")\n",
    "print(f\"最大差异: {torch.max(torch.abs(masked_scores_broadcast - masked_scores_manual)):.10f}\")\n",
    "\n",
    "print(\"\\n=== 总结 ===\")\n",
    "print(\"✓ 所有broadcasting运算结果都与手动编程结果完全一致\")\n",
    "print(\"✓ 最大差异都在机器精度范围内（~1e-10）\")\n",
    "print(\"✓ 这证明了broadcasting机制的正确性和可靠性\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35f54224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 广播失败示例 ---\n",
      "广播失败: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1\n",
      "\n",
      "--- 广播规则总结 ---\n",
      "1. 从右向左对齐维度\n",
      "2. 维度必须兼容：相等、其中一个为1、或其中一个不存在\n",
      "3. 不兼容的维度会自动扩展\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. 广播失败的情况\n",
    "print(\"\\n--- 广播失败示例 ---\")\n",
    "try:\n",
    "    A = torch.randn(3, 4)\n",
    "    B = torch.randn(5)  # 不兼容的维度\n",
    "    C = A + B\n",
    "except RuntimeError as e:\n",
    "    print(f\"广播失败: {e}\")\n",
    "\n",
    "print(\"\\n--- 广播规则总结 ---\")\n",
    "print(\"1. 从右向左对齐维度\")\n",
    "print(\"2. 维度必须兼容：相等、其中一个为1、或其中一个不存在\")\n",
    "print(\"3. 不兼容的维度会自动扩展\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ced67b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== view() vs reshape() 对比 ===\n",
      "原始tensor形状: torch.Size([2, 3, 4])\n",
      "原始tensor是否连续: True\n"
     ]
    }
   ],
   "source": [
    "# 1. 形状变换操作：view vs reshape\n",
    "\n",
    "print(\"=== view() vs reshape() 对比 ===\")\n",
    "import torch\n",
    "\n",
    "# 创建示例tensor\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(f\"原始tensor形状: {x.shape}\")\n",
    "print(f\"原始tensor是否连续: {x.is_contiguous()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "895df05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view(6, 4)后形状: torch.Size([6, 4])\n",
      "view后是否连续: True\n",
      "reshape(6, 4)后形状: torch.Size([6, 4])\n",
      "reshape后是否连续: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# view() 操作\n",
    "y1 = x.view(6, 4)  # 2*3=6\n",
    "print(f\"view(6, 4)后形状: {y1.shape}\")\n",
    "print(f\"view后是否连续: {y1.is_contiguous()}\")\n",
    "\n",
    "# reshape() 操作\n",
    "y2 = x.reshape(6, 4)\n",
    "print(f\"reshape(6, 4)后形状: {y2.shape}\")\n",
    "print(f\"reshape后是否连续: {y2.is_contiguous()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a54bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 不连续tensor的处理 ===\n",
      "转置后是否连续: False\n",
      "view()报错: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 不连续tensor的处理 ===\")\n",
    "# 转置操作会使tensor不连续\n",
    "x_transposed = x.transpose(0, 1)\n",
    "print(f\"转置后是否连续: {x_transposed.is_contiguous()}\")\n",
    "\n",
    "# view() 在不连续tensor上会报错\n",
    "try:\n",
    "    y3 = x_transposed.view(12, 2)\n",
    "except RuntimeError as e:\n",
    "    print(f\"view()报错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79b325be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape()成功: torch.Size([12, 2])\n",
      "\n",
      "=== 内存共享验证 ===\n",
      "修改原始tensor后，view结果: 999.0\n",
      "修改原始tensor后，reshape结果: 999.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# reshape() 会自动处理连续性问题\n",
    "y4 = x_transposed.reshape(12, 2)\n",
    "print(f\"reshape()成功: {y4.shape}\")\n",
    "\n",
    "print(\"\\n=== 内存共享验证 ===\")\n",
    "# 验证view()是否共享内存\n",
    "x[0, 0, 0] = 999\n",
    "print(f\"修改原始tensor后，view结果: {y1[0, 0]}\")\n",
    "print(f\"修改原始tensor后，reshape结果: {y2[0, 0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b343970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== transpose() - 交换两个维度 ===\n",
      "原始tensor形状: torch.Size([2, 3, 4, 5])\n",
      "transpose(1, 3)后形状: torch.Size([2, 5, 4, 3])\n",
      "原始tensor[0, 1, 2, 3] = -1.43731689453125\n",
      "转置后tensor[0, 3, 2, 1] = -1.43731689453125\n",
      "两者是否相等: True\n"
     ]
    }
   ],
   "source": [
    "# 2. 转置操作：transpose vs permute\n",
    "\n",
    "print(\"=== transpose() - 交换两个维度 ===\")\n",
    "x = torch.randn(2, 3, 4, 5)\n",
    "print(f\"原始tensor形状: {x.shape}\")\n",
    "\n",
    "# transpose() 交换维度1和3\n",
    "y1 = x.transpose(1, 3)\n",
    "print(f\"transpose(1, 3)后形状: {y1.shape}\")\n",
    "\n",
    "# 验证转置效果\n",
    "print(f\"原始tensor[0, 1, 2, 3] = {x[0, 1, 2, 3]}\")\n",
    "print(f\"转置后tensor[0, 3, 2, 1] = {y1[0, 3, 2, 1]}\")\n",
    "print(f\"两者是否相等: {x[0, 1, 2, 3] == y1[0, 3, 2, 1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9449d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== permute() - 重新排列所有维度 ===\n",
      "permute(0, 3, 1, 2)后形状: torch.Size([2, 5, 3, 4])\n",
      "原始tensor[0, 1, 2, 3] = -1.43731689453125\n",
      "permute后tensor[0, 3, 1, 2] = -1.43731689453125\n",
      "两者是否相等: True\n",
      "\n",
      "=== 连续性问题 ===\n",
      "原始tensor是否连续: True\n",
      "transpose后是否连续: False\n",
      "permute后是否连续: False\n",
      "contiguous()后是否连续: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== permute() - 重新排列所有维度 ===\")\n",
    "# permute() 重新排列维度\n",
    "y2 = x.permute(0, 3, 1, 2)\n",
    "print(f\"permute(0, 3, 1, 2)后形状: {y2.shape}\")\n",
    "\n",
    "# 验证permute效果\n",
    "print(f\"原始tensor[0, 1, 2, 3] = {x[0, 1, 2, 3]}\")\n",
    "print(f\"permute后tensor[0, 3, 1, 2] = {y2[0, 3, 1, 2]}\")\n",
    "print(f\"两者是否相等: {x[0, 1, 2, 3] == y2[0, 3, 1, 2]}\")\n",
    "\n",
    "print(\"\\n=== 连续性问题 ===\")\n",
    "print(f\"原始tensor是否连续: {x.is_contiguous()}\")\n",
    "print(f\"transpose后是否连续: {y1.is_contiguous()}\")\n",
    "print(f\"permute后是否连续: {y2.is_contiguous()}\")\n",
    "\n",
    "# 如果需要连续tensor，可以调用contiguous()\n",
    "y1_cont = y1.contiguous()\n",
    "print(f\"contiguous()后是否连续: {y1_cont.is_contiguous()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceb30465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeeze() - 移除大小为1的维度 ===\n",
      "原始tensor形状: torch.Size([1, 3, 1, 4])\n",
      "squeeze()后形状: torch.Size([3, 4])\n",
      "squeeze(0)后形状: torch.Size([3, 1, 4])\n",
      "squeeze(2)后形状: torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 3. 维度操作：squeeze vs unsqueeze\n",
    "\n",
    "print(\"=== squeeze() - 移除大小为1的维度 ===\")\n",
    "x = torch.randn(1, 3, 1, 4)\n",
    "print(f\"原始tensor形状: {x.shape}\")\n",
    "\n",
    "# squeeze() 移除所有大小为1的维度\n",
    "y1 = x.squeeze()\n",
    "print(f\"squeeze()后形状: {y1.shape}\")\n",
    "\n",
    "# squeeze(dim) 只移除指定维度\n",
    "y2 = x.squeeze(0)  # 只移除第0维\n",
    "print(f\"squeeze(0)后形状: {y2.shape}\")\n",
    "\n",
    "y3 = x.squeeze(2)  # 只移除第2维\n",
    "print(f\"squeeze(2)后形状: {y3.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f79fe41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== unsqueeze() - 在指定位置插入大小为1的维度 ===\n",
      "原始tensor形状: torch.Size([3, 4])\n",
      "unsqueeze(0)后形状: torch.Size([1, 3, 4])\n",
      "unsqueeze(-1)后形状: torch.Size([3, 4, 1])\n",
      "unsqueeze(1)后形状: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== unsqueeze() - 在指定位置插入大小为1的维度 ===\")\n",
    "x = torch.randn(3, 4)\n",
    "print(f\"原始tensor形状: {x.shape}\")\n",
    "\n",
    "# unsqueeze() 在指定位置插入维度\n",
    "y1 = x.unsqueeze(0)  # 在第0维插入\n",
    "print(f\"unsqueeze(0)后形状: {y1.shape}\")\n",
    "\n",
    "y2 = x.unsqueeze(-1)  # 在最后一维插入\n",
    "print(f\"unsqueeze(-1)后形状: {y2.shape}\")\n",
    "\n",
    "y3 = x.unsqueeze(1)  # 在第1维插入\n",
    "print(f\"unsqueeze(1)后形状: {y3.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b5ad874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gather() - 按索引收集元素 ===\n",
      "原始tensor:\n",
      "tensor([[ 0.3435,  0.6903,  1.0170, -0.5715],\n",
      "        [ 1.4580,  1.1166,  0.1565, -0.3784],\n",
      "        [-2.4508, -1.1759,  2.2299,  1.1512]])\n",
      "索引: tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 6. 索引和选择操作\n",
    "\n",
    "print(\"=== gather() - 按索引收集元素 ===\")\n",
    "# 创建示例tensor\n",
    "x = torch.randn(3, 4)\n",
    "print(f\"原始tensor:\\n{x}\")\n",
    "\n",
    "# 创建索引\n",
    "indices = torch.tensor([0, 2, 1])\n",
    "print(f\"索引: {indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7b7ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather结果:\n",
      "tensor([[ 0.3435],\n",
      "        [ 0.1565],\n",
      "        [-1.1759]])\n",
      "解释: 每行按indices选择元素\n",
      "第0行选择第0列: 0.34346070885658264\n",
      "第1行选择第2列: 0.15652576088905334\n",
      "第2行选择第1列: -1.175888180732727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 在第1维上按索引收集元素\n",
    "y = torch.gather(x, 1, indices.unsqueeze(1))\n",
    "print(f\"gather结果:\\n{y}\")\n",
    "print(f\"解释: 每行按indices选择元素\")\n",
    "\n",
    "# 验证结果\n",
    "print(f\"第0行选择第{indices[0]}列: {x[0, indices[0]]}\")\n",
    "print(f\"第1行选择第{indices[1]}列: {x[1, indices[1]]}\")\n",
    "print(f\"第2行选择第{indices[2]}列: {x[2, indices[2]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b21c5d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== scatter() - 按索引分散元素 ===\n",
      "目标tensor:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "要分散的值:\n",
      "tensor([[ 0.9913, -0.8953],\n",
      "        [-1.0821,  0.8034],\n",
      "        [-0.0603, -0.0386]])\n",
      "分散索引:\n",
      "tensor([[0, 2],\n",
      "        [1, 3],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== scatter() - 按索引分散元素 ===\")\n",
    "# 创建目标tensor\n",
    "x_scatter = torch.zeros(3, 4)\n",
    "values = torch.randn(3, 2)\n",
    "indices_scatter = torch.tensor([[0, 2], [1, 3], [0, 1]])\n",
    "print(f\"目标tensor:\\n{x_scatter}\")\n",
    "print(f\"要分散的值:\\n{values}\")\n",
    "print(f\"分散索引:\\n{indices_scatter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69968f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scatter后结果:\n",
      "tensor([[ 0.9913,  0.0000, -0.8953,  0.0000],\n",
      "        [ 0.0000, -1.0821,  0.0000,  0.8034],\n",
      "        [-0.0603, -0.0386,  0.0000,  0.0000]])\n",
      "\n",
      "=== index_select() - 按索引选择 ===\n",
      "原始tensor形状: torch.Size([5, 3])\n",
      "选择的索引: tensor([0, 2, 4])\n",
      "选择结果形状: torch.Size([3, 3])\n",
      "选择结果:\n",
      "tensor([[-0.9093, -0.1662,  0.6354],\n",
      "        [ 0.3478, -0.7638,  0.6484],\n",
      "        [-1.5842,  1.6928,  0.2203]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 分散元素\n",
    "x_scatter.scatter_(1, indices_scatter, values)\n",
    "print(f\"scatter后结果:\\n{x_scatter}\")\n",
    "\n",
    "print(\"\\n=== index_select() - 按索引选择 ===\")\n",
    "# 按索引选择行\n",
    "x = torch.randn(5, 3)\n",
    "indices = torch.tensor([0, 2, 4])\n",
    "selected = torch.index_select(x, 0, indices)\n",
    "print(f\"原始tensor形状: {x.shape}\")\n",
    "print(f\"选择的索引: {indices}\")\n",
    "print(f\"选择结果形状: {selected.shape}\")\n",
    "print(f\"选择结果:\\n{selected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74179540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== masked_select() - 按掩码选择 ===\n",
      "原始tensor:\n",
      "tensor([[-0.1824,  0.3434, -1.7025, -1.3546],\n",
      "        [-0.0932, -1.2863,  0.3139,  1.1156],\n",
      "        [-0.1857,  1.6313, -1.2955, -0.0053]])\n",
      "掩码 (>0.5):\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False,  True],\n",
      "        [False,  True, False, False]])\n",
      "按掩码选择的值: tensor([1.1156, 1.6313])\n",
      "选择的值数量: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== masked_select() - 按掩码选择 ===\")\n",
    "# 创建掩码\n",
    "x = torch.randn(3, 4)\n",
    "mask = x > 0.5\n",
    "print(f\"原始tensor:\\n{x}\")\n",
    "print(f\"掩码 (>0.5):\\n{mask}\")\n",
    "\n",
    "# 按掩码选择\n",
    "selected_values = torch.masked_select(x, mask)\n",
    "print(f\"按掩码选择的值: {selected_values}\")\n",
    "print(f\"选择的值数量: {len(selected_values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "099021d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c963e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
