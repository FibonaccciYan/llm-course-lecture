{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第2讲: 特征空间的变换 - 前反向运行视角理解深度学习模型\n",
        "\n",
        "本notebook将演示深度学习模型的核心概念：\n",
        "- Tensor基础操作\n",
        "- 矩阵乘法和空间变换\n",
        "- 线性层的构建和使用\n",
        "- 反向传播和自动求导\n",
        "- 完整的训练循环\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 导入必要的库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch版本: 2.8.0\n",
            "CUDA可用: False\n",
            "MPS可用: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设置随机种子以便结果可复现\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
        "print(f\"MPS可用: {torch.backends.mps.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tensor基础概念\n",
        "\n",
        "Tensor是PyTorch中最基础的数据结构，可以理解为多维数组。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Tensor维度演示 ===\n",
            "标量: 3.140000104904175, shape: torch.Size([])\n",
            "向量: tensor([1, 2, 3, 4]), shape: torch.Size([4])\n",
            "矩阵: tensor([[1, 2],\n",
            "        [3, 4]]), shape: torch.Size([2, 2])\n",
            "高维Tensor: shape: torch.Size([2, 3, 4, 5])\n",
            "随机Tensor: tensor([[-0.2387, -0.5050, -2.4752, -0.9316],\n",
            "        [-0.1335,  0.3415, -0.0716, -0.0909],\n",
            "        [-1.3297, -0.5426,  0.5471,  0.6431]])\n"
          ]
        }
      ],
      "source": [
        "# 创建不同维度的Tensor\n",
        "print(\"=== Tensor维度演示 ===\")\n",
        "\n",
        "# 标量 (0维)\n",
        "scalar = torch.tensor(3.14)\n",
        "print(f\"标量: {scalar}, shape: {scalar.shape}\")\n",
        "\n",
        "# 向量 (1维)\n",
        "vector = torch.tensor([1, 2, 3, 4])\n",
        "print(f\"向量: {vector}, shape: {vector.shape}\")\n",
        "\n",
        "# 矩阵 (2维)\n",
        "matrix = torch.tensor([[1, 2], [3, 4]])\n",
        "print(f\"矩阵: {matrix}, shape: {matrix.shape}\")\n",
        "\n",
        "# 高维Tensor (常用于输入输出/激活)\n",
        "high_dim = torch.randn(2, 3, 4, 5)\n",
        "print(f\"高维Tensor: shape: {high_dim.shape}\")\n",
        "\n",
        "# 随机初始化\n",
        "random_tensor = torch.randn(3, 4)\n",
        "print(f\"随机Tensor: {random_tensor}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 矩阵乘法操作\n",
        "\n",
        "深度学习模型的核心是矩阵乘法，实现特征空间的变换：$Y = XW + b$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 矩阵乘法演示 ===\n",
            "输入 X: tensor([[ 2.9246, -0.7985, -0.5669],\n",
            "        [-0.0267, -1.5460, -2.1799]])\n",
            "X shape: torch.Size([2, 3])\n",
            "权重 W: tensor([[ 0.2074, -1.9844, -0.1817,  0.4280],\n",
            "        [ 0.2545,  0.0662, -1.2704,  0.6674],\n",
            "        [-1.0002, -0.0244,  0.2497, -1.7517]])\n",
            "W shape: torch.Size([3, 4])\n",
            "偏置 b: tensor([-0.1309,  1.4378, -0.1544, -0.2853])\n",
            "b shape: torch.Size([4])\n",
            "\n",
            "方法1 - @ 操作符结果: tensor([[ 0.8393, -4.4045,  0.1872,  1.4264],\n",
            "        [ 1.6504,  1.4418,  1.2702,  2.4900]])\n",
            "Y1 shape: torch.Size([2, 4])\n",
            "\n",
            "方法2 - torch.matmul结果: tensor([[ 0.8393, -4.4045,  0.1872,  1.4264],\n",
            "        [ 1.6504,  1.4418,  1.2702,  2.4900]])\n",
            "\n",
            "方法3 - tensor.matmul结果: tensor([[ 0.8393, -4.4045,  0.1872,  1.4264],\n",
            "        [ 1.6504,  1.4418,  1.2702,  2.4900]])\n",
            "\n",
            "结果是否相同: True\n"
          ]
        }
      ],
      "source": [
        "print(\"=== 矩阵乘法演示 ===\")\n",
        "\n",
        "# 创建输入矩阵 X (batch_size=2, input_dim=3)\n",
        "X = torch.randn(2, 3)\n",
        "print(f\"输入 X: {X}\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "\n",
        "# 创建权重矩阵 W (input_dim=3, output_dim=4)\n",
        "W = torch.randn(3, 4)\n",
        "print(f\"权重 W: {W}\")\n",
        "print(f\"W shape: {W.shape}\")\n",
        "\n",
        "# 创建偏置 b (output_dim=4)\n",
        "b = torch.randn(4)\n",
        "print(f\"偏置 b: {b}\")\n",
        "print(f\"b shape: {b.shape}\")\n",
        "\n",
        "# 方法1: 使用 @ 操作符\n",
        "Y1 = X @ W + b\n",
        "print(f\"\\n方法1 - @ 操作符结果: {Y1}\")\n",
        "print(f\"Y1 shape: {Y1.shape}\")\n",
        "\n",
        "# 方法2: 使用 torch.matmul\n",
        "Y2 = torch.matmul(X, W) + b\n",
        "print(f\"\\n方法2 - torch.matmul结果: {Y2}\")\n",
        "\n",
        "# 方法3: 使用 tensor.matmul\n",
        "Y3 = X.matmul(W) + b\n",
        "print(f\"\\n方法3 - tensor.matmul结果: {Y3}\")\n",
        "\n",
        "# 验证结果是否相同\n",
        "print(f\"\\n结果是否相同: {torch.allclose(Y1, Y2) and torch.allclose(Y2, Y3)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. einsum操作\n",
        "\n",
        "einsum使用爱因斯坦求和约定，可以直观地描述张量运算。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== einsum操作演示 ===\n",
            "einsum结果: tensor([[-0.9760, -0.2255, -1.2338, -1.6805],\n",
            "        [ 1.3851,  0.7489,  0.5305, -1.9812]])\n",
            "matmul结果: tensor([[-0.9760, -0.2255, -1.2338, -1.6805],\n",
            "        [ 1.3851,  0.7489,  0.5305, -1.9812]])\n",
            "结果相同: True\n"
          ]
        }
      ],
      "source": [
        "print(\"=== einsum操作演示 ===\")\n",
        "\n",
        "# 矩阵乘法 Y = XW 的einsum表示\n",
        "X = torch.randn(2, 3)  # batch=2, input_dim=3\n",
        "W = torch.randn(3, 4)  # input_dim=3, output_dim=4\n",
        "\n",
        "# 数学形式: Y_{be} = sum_d X_{bd} * W_{de}\n",
        "Y_einsum = torch.einsum(\"bd,de->be\", X, W)\n",
        "Y_matmul = X @ W\n",
        "\n",
        "print(f\"einsum结果: {Y_einsum}\")\n",
        "print(f\"matmul结果: {Y_matmul}\")\n",
        "print(f\"结果相同: {torch.allclose(Y_einsum, Y_matmul)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b1_elementwise_mul: tensor([[ 2.6000, -2.3622, -4.1707,  0.7482, -6.6441],\n",
            "        [-0.6081,  0.0969,  3.0589, -0.7106,  2.9023],\n",
            "        [-0.0136,  0.3155, -1.6328,  0.0387, -1.5945]])\n",
            "Y_batch[0]: tensor([[ 2.6000, -2.3622, -4.1707,  0.7482, -6.6441],\n",
            "        [-0.6081,  0.0969,  3.0589, -0.7106,  2.9023],\n",
            "        [-0.0136,  0.3155, -1.6328,  0.0387, -1.5945]])\n",
            "Y_batch_bmm[0]: tensor([[ 2.6000, -2.3622, -4.1707,  0.7482, -6.6441],\n",
            "        [-0.6081,  0.0969,  3.0589, -0.7106,  2.9023],\n",
            "        [-0.0136,  0.3155, -1.6328,  0.0387, -1.5945]])\n",
            "结果相同: True\n",
            "torch.bmm结果shape: torch.Size([10, 3, 5])\n",
            "einsum和bmm结果相同: True\n",
            "\n",
            "批量矩阵乘法结果shape: torch.Size([10, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "# 批量矩阵乘法\n",
        "A = torch.randn(10, 3, 4)  # batch=10\n",
        "B = torch.randn(10, 4, 5)\n",
        "\n",
        "# 数学形式: Y_{bij} = sum_k A_{bik} * B_{bkj}\n",
        "Y_batch = torch.einsum(\"bik,bkj->bij\", A, B)\n",
        "# 用torch.bmm实现批量矩阵乘法（需要保证A, B的shape为[batch, n, m]和[batch, m, p]）\n",
        "Y_batch_bmm = torch.bmm(A, B)\n",
        "b1_elementwise_mul = A[0] @ B[0]\n",
        "\n",
        "print(f\"b1_elementwise_mul: {b1_elementwise_mul}\")\n",
        "print(f\"Y_batch[0]: {Y_batch[0]}\")\n",
        "print(f\"Y_batch_bmm[0]: {Y_batch_bmm[0]}\")\n",
        "print(f\"结果相同: {torch.allclose(b1_elementwise_mul, Y_batch[0]) and torch.allclose(b1_elementwise_mul, Y_batch_bmm[0])}\")\n",
        "\n",
        "print(f\"torch.bmm结果shape: {Y_batch_bmm.shape}\")\n",
        "print(f\"einsum和bmm结果相同: {torch.allclose(Y_batch, Y_batch_bmm)}\")\n",
        "print(f\"\\n批量矩阵乘法结果shape: {Y_batch.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "元素乘法 - einsum: tensor([[ 0.0103, -0.0192, -0.9273],\n",
            "        [-0.4390, -0.2977, -0.3999]])\n",
            "元素乘法 - 手动: tensor([[ 0.0103, -0.0192, -0.9273],\n",
            "        [-0.4390, -0.2977, -0.3999]])\n",
            "结果相同: True\n",
            "\n",
            "点积 - einsum: -1.4463235139846802\n",
            "点积 - 手动: -1.4463235139846802\n"
          ]
        }
      ],
      "source": [
        "# 元素乘法（Hadamard积）演示\n",
        "A = torch.randn(2, 3)\n",
        "B = torch.randn(2, 3)\n",
        "elementwise_mul = torch.einsum(\"ij,ij->ij\", A, B)\n",
        "elementwise_mul_manual = A * B\n",
        "print(f\"\\n元素乘法 - einsum: {elementwise_mul}\")\n",
        "print(f\"元素乘法 - 手动: {elementwise_mul_manual}\")\n",
        "print(f\"结果相同: {torch.allclose(elementwise_mul, elementwise_mul_manual)}\")\n",
        "\n",
        "# 点积\n",
        "a = torch.randn(3)\n",
        "b = torch.randn(3)\n",
        "dot_product = torch.einsum(\"i,i->\", a, b)\n",
        "dot_product_manual = torch.sum(a * b)\n",
        "print(f\"\\n点积 - einsum: {dot_product}\")\n",
        "print(f\"点积 - 手动: {dot_product_manual}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. nn.Linear层演示\n",
        "\n",
        "nn.Linear是深度学习模型的基础\"积木\"，实现线性变换：$Y = XW + b$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== nn.Linear层演示 ===\n",
            "线性层: Linear(in_features=3, out_features=4, bias=True)\n",
            "权重shape: torch.Size([4, 3])\n",
            "偏置shape: torch.Size([4])\n",
            "权重: tensor([[-0.5600, -0.2996,  0.3952],\n",
            "        [-0.5436, -0.5025,  0.3234],\n",
            "        [ 0.3115,  0.4748, -0.4359],\n",
            "        [-0.4226,  0.2962,  0.5021]])\n",
            "偏置: tensor([0.3455, 0.0904, 0.1903, 0.5480])\n",
            "\n",
            "输入 x: tensor([[-1.5626,  0.8695,  0.2145],\n",
            "        [-0.7496, -0.4951,  1.3849]])\n",
            "x shape: torch.Size([2, 3])\n",
            "\n",
            "输出: tensor([[ 1.0448,  0.5722,  0.0229,  1.5735],\n",
            "        [ 1.4609,  1.1946, -0.8819,  1.4134]], grad_fn=<AddmmBackward0>)\n",
            "输出shape: torch.Size([2, 4])\n",
            "\n",
            "手动计算结果: tensor([[ 1.0448,  0.5722,  0.0229,  1.5735],\n",
            "        [ 1.4609,  1.1946, -0.8819,  1.4134]], grad_fn=<AddBackward0>)\n",
            "结果相同: True\n"
          ]
        }
      ],
      "source": [
        "print(\"=== nn.Linear层演示 ===\")\n",
        "\n",
        "# 创建线性层\n",
        "linear_layer = nn.Linear(in_features=3, out_features=4, bias=True)\n",
        "print(f\"线性层: {linear_layer}\")\n",
        "print(f\"权重shape: {linear_layer.weight.shape}\")\n",
        "print(f\"偏置shape: {linear_layer.bias.shape}\")\n",
        "print(f\"权重: {linear_layer.weight.data}\")\n",
        "print(f\"偏置: {linear_layer.bias.data}\")\n",
        "\n",
        "# 创建输入\n",
        "x = torch.randn(2, 3)  # batch_size=2, input_dim=3\n",
        "print(f\"\\n输入 x: {x}\")\n",
        "print(f\"x shape: {x.shape}\")\n",
        "\n",
        "# 前向传播\n",
        "output = linear_layer(x)\n",
        "print(f\"\\n输出: {output}\")\n",
        "print(f\"输出shape: {output.shape}\")\n",
        "\n",
        "# 手动计算验证\n",
        "\n",
        "manual_output = x @ linear_layer.weight.T + linear_layer.bias\n",
        "print(f\"\\n手动计算结果: {manual_output}\")\n",
        "print(f\"结果相同: {torch.allclose(output, manual_output)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 多层线性层堆叠\n",
        "\n",
        "演示如何将多个线性层组合成更复杂的模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 多层线性层堆叠演示 ===\n",
            "第一层: Linear(in_features=20, out_features=30, bias=True)\n",
            "第二层: Linear(in_features=30, out_features=40, bias=True)\n",
            "\n",
            "输入 x shape: torch.Size([128, 20])\n",
            "第一层输出 y1 shape: torch.Size([128, 30])\n",
            "第二层输出 y2 shape: torch.Size([128, 40])\n"
          ]
        }
      ],
      "source": [
        "print(\"=== 多层线性层堆叠演示 ===\")\n",
        "\n",
        "# 创建多层线性层\n",
        "layer1 = nn.Linear(20, 30)\n",
        "layer2 = nn.Linear(30, 40)\n",
        "\n",
        "print(f\"第一层: {layer1}\")\n",
        "print(f\"第二层: {layer2}\")\n",
        "\n",
        "# 创建输入\n",
        "x = torch.randn(128, 20)  # batch_size=128, input_dim=20\n",
        "print(f\"\\n输入 x shape: {x.shape}\")\n",
        "\n",
        "# 逐层前向传播\n",
        "y1 = layer1(x)  # Y1 = XW1\n",
        "print(f\"第一层输出 y1 shape: {y1.shape}\")\n",
        "\n",
        "y2 = layer2(y1)  # Y2 = Y1W2\n",
        "print(f\"第二层输出 y2 shape: {y2.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "高维输入 x_high_dim shape: torch.Size([128, 4096, 30, 20])\n",
            "高维输入第一层输出 shape: torch.Size([128, 4096, 30, 30])\n",
            "高维输入第二层输出 shape: torch.Size([128, 4096, 30, 40])\n"
          ]
        }
      ],
      "source": [
        "# 验证高维输入\n",
        "x_high_dim = torch.randn(128, 4096, 30, 20)\n",
        "print(f\"\\n高维输入 x_high_dim shape: {x_high_dim.shape}\")\n",
        "\n",
        "# nn.Linear会自动处理高维输入，只对最后两个维度进行线性变换\n",
        "y1_high = layer1(x_high_dim)\n",
        "print(f\"高维输入第一层输出 shape: {y1_high.shape}\")\n",
        "\n",
        "y2_high = layer2(y1_high)\n",
        "print(f\"高维输入第二层输出 shape: {y2_high.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 反向传播和自动求导\n",
        "\n",
        "演示PyTorch的自动求导机制，这是深度学习训练的核心。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 反向传播和自动求导演示 ===\n",
            "输入 x: 2.0\n",
            "权重 w: 3.0\n",
            "偏置 b: 1.0\n",
            "\n",
            "前向传播结果 z: 7.0\n",
            "目标值: 10.0\n",
            "损失值: 9.0\n",
            "\n",
            "梯度信息:\n",
            "∂loss/∂x = -18.0\n",
            "∂loss/∂w = -12.0\n",
            "∂loss/∂b = -6.0\n",
            "\n",
            "手动计算的梯度:\n",
            "∂loss/∂x = -18.0\n",
            "∂loss/∂w = -12.0\n",
            "∂loss/∂b = -6.0\n",
            "\n",
            "梯度计算正确: True\n"
          ]
        }
      ],
      "source": [
        "print(\"=== 反向传播和自动求导演示 ===\")\n",
        "\n",
        "# 创建需要梯度的参数\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "print(f\"输入 x: {x}\")\n",
        "print(f\"权重 w: {w}\")\n",
        "print(f\"偏置 b: {b}\")\n",
        "\n",
        "# 前向传播: z = x * w + b\n",
        "z = x * w + b\n",
        "print(f\"\\n前向传播结果 z: {z}\")\n",
        "\n",
        "# 定义损失函数 (这里用简单的平方损失)\n",
        "target = torch.tensor(10.0)\n",
        "loss = (z - target) ** 2\n",
        "print(f\"目标值: {target}\")\n",
        "print(f\"损失值: {loss}\")\n",
        "\n",
        "# 反向传播\n",
        "loss.backward()\n",
        "\n",
        "# 查看梯度\n",
        "print(f\"\\n梯度信息:\")\n",
        "print(f\"∂loss/∂x = {x.grad}\")\n",
        "print(f\"∂loss/∂w = {w.grad}\")\n",
        "print(f\"∂loss/∂b = {b.grad}\")\n",
        "\n",
        "# 手动验证梯度计算\n",
        "# loss = (z - target)^2 = (x*w + b - target)^2\n",
        "# ∂loss/∂x = 2*(x*w + b - target) * w = 2*(z - target) * w\n",
        "# ∂loss/∂w = 2*(x*w + b - target) * x = 2*(z - target) * x\n",
        "# ∂loss/∂b = 2*(x*w + b - target) * 1 = 2*(z - target)\n",
        "\n",
        "manual_grad_x = 2 * (z - target) * w\n",
        "manual_grad_w = 2 * (z - target) * x\n",
        "manual_grad_b = 2 * (z - target)\n",
        "\n",
        "print(f\"\\n手动计算的梯度:\")\n",
        "print(f\"∂loss/∂x = {manual_grad_x}\")\n",
        "print(f\"∂loss/∂w = {manual_grad_w}\")\n",
        "print(f\"∂loss/∂b = {manual_grad_b}\")\n",
        "\n",
        "print(f\"\\n梯度计算正确: {torch.allclose(x.grad, manual_grad_x) and torch.allclose(w.grad, manual_grad_w) and torch.allclose(b.grad, manual_grad_b)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 完整的训练循环演示\n",
        "\n",
        "演示一个完整的深度学习模型训练过程。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 完整训练循环演示 ===\n",
            "模型: SimpleModel(\n",
            "  (linear1): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "训练数据 X shape: torch.Size([100, 2])\n",
            "训练数据 y shape: torch.Size([100, 1])\n",
            "Epoch [20/100], Loss: 9.0226\n",
            "Epoch [40/100], Loss: 1.9250\n",
            "Epoch [60/100], Loss: 0.2798\n",
            "Epoch [80/100], Loss: 0.1892\n",
            "Epoch [100/100], Loss: 0.1777\n",
            "\n",
            "最终损失: 0.1777\n",
            "\n",
            "测试输入: tensor([[1., 2.]])\n",
            "模型预测: 7.9833\n",
            "期望输出: 9.0000\n",
            "预测误差: 1.0167\n"
          ]
        }
      ],
      "source": [
        "print(\"=== 完整训练循环演示 ===\")\n",
        "\n",
        "# 创建简单的回归模型\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# 创建模型\n",
        "model = SimpleModel(input_dim=2, hidden_dim=10, output_dim=1)\n",
        "print(f\"模型: {model}\")\n",
        "\n",
        "# 创建损失函数和优化器\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 生成简单的训练数据 (y = 2*x1 + 3*x2 + 1 + noise)\n",
        "torch.manual_seed(42)\n",
        "X_train = torch.randn(100, 2)\n",
        "y_train = 2 * X_train[:, 0] + 3 * X_train[:, 1] + 1 + 0.1 * torch.randn(100)\n",
        "y_train = y_train.unsqueeze(1)  # 添加维度\n",
        "\n",
        "print(f\"训练数据 X shape: {X_train.shape}\")\n",
        "print(f\"训练数据 y shape: {y_train.shape}\")\n",
        "\n",
        "# 训练循环\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 前向传播\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    \n",
        "    # 反向传播\n",
        "    optimizer.zero_grad()  # 清零梯度\n",
        "    loss.backward()        # 计算梯度\n",
        "    optimizer.step()       # 更新参数\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f\"\\n最终损失: {losses[-1]:.4f}\")\n",
        "\n",
        "# 测试模型\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([[1.0, 2.0]])\n",
        "    test_output = model(test_input)\n",
        "    expected = 2 * 1.0 + 3 * 2.0 + 1  # 应该是9\n",
        "    print(f\"\\n测试输入: {test_input}\")\n",
        "    print(f\"模型预测: {test_output.item():.4f}\")\n",
        "    print(f\"期望输出: {expected:.4f}\")\n",
        "    print(f\"预测误差: {abs(test_output.item() - expected):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
