{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetwork(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      ")\n",
      "<bound method Module.parameters of MyNetwork(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      ")>\n",
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = MyNetwork()\n",
    "print(model)\n",
    "print(model.parameters)\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias'])\n",
      "tensor([[[-0.0890,  0.0042, -0.0427,  0.0794, -0.0911],\n",
      "         [-0.0134,  0.0673,  0.1109,  0.0731, -0.0212],\n",
      "         [-0.0748, -0.0502, -0.0151, -0.0158,  0.0622],\n",
      "         [ 0.0674,  0.0024,  0.0136,  0.0365, -0.1080],\n",
      "         [-0.0154,  0.0232,  0.0612,  0.0337,  0.0293]],\n",
      "\n",
      "        [[ 0.0887, -0.0952,  0.0437,  0.0708, -0.0881],\n",
      "         [-0.0270, -0.0858,  0.1125, -0.0995,  0.0084],\n",
      "         [-0.0762,  0.0590, -0.0690, -0.0739, -0.0135],\n",
      "         [ 0.1025, -0.0979, -0.1078,  0.1074, -0.0003],\n",
      "         [ 0.0081,  0.0468,  0.0784, -0.1064,  0.0047]],\n",
      "\n",
      "        [[ 0.0773,  0.0785,  0.0373,  0.1052, -0.0265],\n",
      "         [ 0.0410, -0.0077,  0.1055,  0.0623,  0.0079],\n",
      "         [-0.1011, -0.0397,  0.0509, -0.0085,  0.0571],\n",
      "         [ 0.0295, -0.0917, -0.0098, -0.0709,  0.0425],\n",
      "         [ 0.0484,  0.0249, -0.0834, -0.0722, -0.0796]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[[-0.0890,  0.0042, -0.0427,  0.0794, -0.0911],\n",
      "         [-0.0134,  0.0673,  0.1109,  0.0731, -0.0212],\n",
      "         [-0.0748, -0.0502, -0.0151, -0.0158,  0.0622],\n",
      "         [ 0.0674,  0.0024,  0.0136,  0.0365, -0.1080],\n",
      "         [-0.0154,  0.0232,  0.0612,  0.0337,  0.0293]],\n",
      "\n",
      "        [[ 0.0887, -0.0952,  0.0437,  0.0708, -0.0881],\n",
      "         [-0.0270, -0.0858,  0.1125, -0.0995,  0.0084],\n",
      "         [-0.0762,  0.0590, -0.0690, -0.0739, -0.0135],\n",
      "         [ 0.1025, -0.0979, -0.1078,  0.1074, -0.0003],\n",
      "         [ 0.0081,  0.0468,  0.0784, -0.1064,  0.0047]],\n",
      "\n",
      "        [[ 0.0773,  0.0785,  0.0373,  0.1052, -0.0265],\n",
      "         [ 0.0410, -0.0077,  0.1055,  0.0623,  0.0079],\n",
      "         [-0.1011, -0.0397,  0.0509, -0.0085,  0.0571],\n",
      "         [ 0.0295, -0.0917, -0.0098, -0.0709,  0.0425],\n",
      "         [ 0.0484,  0.0249, -0.0834, -0.0722, -0.0796]]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model_weight.pt')\n",
    "model_new = MyNetwork()\n",
    "model_new.load_state_dict(torch.load('model_weight.pt', weights_only=True))\n",
    "print(model_new.state_dict().keys())\n",
    "print(model_new.conv1.weight[0])\n",
    "print(model.conv1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.97s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "model_id = '/Users/jingweixu/Downloads/Meta-Llama-3.1-8B-Instruct'\n",
    "llama = transformers.LlamaForCausalLM.from_pretrained(model_id)\n",
    "# llama.save_pretrained('/Users/jingweixu/Downloads/llama_test', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "print(llama)\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "peft_model = get_peft_model(llama, peft_config)\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
