# å¤§è¯­è¨€æ¨¡å‹åŸºç¡€ï¼šä»é›¶åˆ°ä¸€å®ç°ä¹‹è·¯

[![Course Badge](https://img.shields.io/badge/Course-LLM%20Basics-blue.svg)](https://njudeepengine.github.io/llm-course-lecture/)
[![Year](https://img.shields.io/badge/Year-2024%2F2025-green.svg)]()
[![Language](https://img.shields.io/badge/Language-Chinese%2FEnglish-yellow.svg)]()

> ä¸€é—¨ç†è®ºä¸å®è·µå¹¶é‡çš„å¤§è¯­è¨€æ¨¡å‹è¯¾ç¨‹ï¼Œå¸¦ä½ ä»é›¶å¼€å§‹ç†è§£å’Œå®ç°LLMçš„å„ä¸ªç»„ä»¶ï¼ˆå†…å®¹ç”±Cursorç”Ÿæˆhhhï¼‰

## ğŸ“š è¯¾ç¨‹ç®€ä»‹

æœ¬è¯¾ç¨‹æ—¨åœ¨æ·±å…¥æµ…å‡ºåœ°è®²è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰çš„åŸºç¡€ç†è®ºå’Œå®è·µå®ç°ã€‚é€šè¿‡"ä»é›¶åˆ°ä¸€"çš„å­¦ä¹ è·¯å¾„ï¼Œä½ å°†ä¸ä»…ç†è§£LLMçš„å·¥ä½œåŸç†ï¼Œè¿˜èƒ½äº²æ‰‹å®ç°å„ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚

### ğŸ¯ è¯¾ç¨‹ç›®æ ‡

- **ç†è®ºåŸºç¡€**ï¼šæŒæ¡LLMçš„æ•°å­¦åŸç†å’Œæ¶æ„è®¾è®¡
- **å®è·µèƒ½åŠ›**ï¼šä»é›¶å¼€å§‹å®ç°LLMçš„å„ä¸ªç»„ä»¶
- **å·¥ç¨‹æŠ€èƒ½**ï¼šå­¦ä¹ PyTorchã€Transformersç­‰ä¸»æµæ¡†æ¶çš„ä½¿ç”¨
- **å‰æ²¿æŠ€æœ¯**ï¼šäº†è§£FlashAttentionã€åˆ†å¸ƒå¼è®­ç»ƒç­‰é«˜çº§æŠ€æœ¯

## ğŸ‘¨â€ğŸ« è¯¾ç¨‹å›¢é˜Ÿ

- **ä¸»è®²æ•™å¸ˆ**ï¼šå¾ç»çº¬
- **åŠå…¬å®¤**ï¼šè®¡ç®—æœºå­¦é™¢1022
- **é‚®ç®±**ï¼šjingweix@nju.edu.cn
- **è¯¾ç¨‹è”åˆåˆ›å§‹äºº**ï¼šé»„äº‘é¹
- **2025åŠ©æ•™å›¢é˜Ÿ**ï¼šèµµä¸–é©¹ã€æ¢æ˜å®‡ã€åœéŸ¬ã€ç‹ä¹¾åˆšã€å¾é¼å¤

## ğŸŒ ç›¸å…³é“¾æ¥

- **è¯¾ç¨‹ä¸»é¡µ**ï¼š[https://njudeepengine.github.io/llm-course-lecture/](https://njudeepengine.github.io/llm-course-lecture/)
- **ä½œä¸šä¸»é¡µ**ï¼š[https://njudeepengine.github.io/LLM-Blog/](https://njudeepengine.github.io/LLM-Blog/)
- **Bç«™è¯¾ç¨‹è§†é¢‘**ï¼š [ç¬¬1è®²](https://www.bilibili.com/video/BV12RemzSEfV)ã€[ç¬¬3è®²](https://www.bilibili.com/video/BV1EJH9zRE2p)
- **QQç¾¤**ï¼š1033682290 (NJUå†…éƒ¨)

## ğŸ“– è¯¾ç¨‹å¤§çº²

### 2025å¹´è¯¾ç¨‹å†…å®¹

1. **è¯¾ç¨‹ç®€ä»‹** - è¯¾ç¨‹æ¦‚è¿°å’Œå­¦ä¹ è·¯å¾„  
   è®²ä¹‰é“¾æ¥: https://njudeepengine.github.io/llm-course-lecture/2025/lecture1.html

   Bç«™é“¾æ¥: https://www.bilibili.com/video/BV12RemzSEfV
2. **ç‰¹å¾ç©ºé—´çš„å˜æ¢1** - å‰åå‘è¿è¡Œè§†è§’ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹  
   è®²ä¹‰é“¾æ¥: https://njudeepengine.github.io/llm-course-lecture/2025/lecture2.html
3. **ç‰¹å¾ç©ºé—´çš„å˜æ¢2** - å‰åå‘è¿è¡Œè§†è§’ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹  
   è®²ä¹‰é“¾æ¥: https://njudeepengine.github.io/llm-course-lecture/2025/lecture3.html
   
   Bç«™é“¾æ¥: https://www.bilibili.com/video/BV1EJH9zRE2p


### 2024å¹´è¯¾ç¨‹å†…å®¹

#### åŸºç¡€ç†è®º
1. **è¯¾ç¨‹ç®€ä»‹** - è¯¾ç¨‹æ¦‚è¿°å’Œå­¦ä¹ è·¯å¾„
2. **æ·±åº¦å­¦ä¹ åŸºç¡€ I** - PyTorchåŸºç¡€å’ŒåŸºæœ¬æ¦‚å¿µ
3. **æ·±åº¦å­¦ä¹ åŸºç¡€ II** - åå‘ä¼ æ’­å’Œä¼˜åŒ–ç®—æ³•
4. **å¤§è¯­è¨€æ¨¡å‹è§£æ I** - Tokenizerå’ŒPositional Embedding
5. **å¤§è¯­è¨€æ¨¡å‹è§£æ II** - RMSNormå’ŒMLP
6. **å¤§è¯­è¨€æ¨¡å‹è§£æ III** - Attention Layer
7. **å¤§è¯­è¨€æ¨¡å‹è§£æ IV** - Flash Attention
8. **å¤§è¯­è¨€æ¨¡å‹è§£æ V** - MoEå’ŒLoRA
9. **å¤§è¯­è¨€æ¨¡å‹è§£æ VI** - Fine-tuning

#### é«˜çº§æŠ€æœ¯
10. **å¤§è¯­è¨€æ¨¡å‹è§£æ VII** - LLM save/load
11. **å¤§è¯­è¨€æ¨¡å‹è§£æ VIII** - LLMæ¨ç†ã€è§£ç ç­–ç•¥ã€KVCache
12. **å¤§è¯­è¨€æ¨¡å‹æ¨ç†æŠ€æœ¯** - RAGåŸºç¡€
13. **å¤§è¯­è¨€æ¨¡å‹æ¨ç†æŠ€æœ¯** - RAGè¿›é˜¶
14. **å¤§è¯­è¨€æ¨¡å‹è¿›é˜¶** - Megatronä¸­çš„å¹¶è¡ŒåŒ–æŠ€æœ¯


## ğŸ“ è¯„åˆ†ä½“ç³»

- **ä»é›¶åˆ°ä¸€ä¹‹è·¯**ï¼š5æ¬¡ç¼–ç¨‹ä½œä¸š (80%)
- **å¯åŠ¨ï¼šå¤§ä½œä¸š**ï¼šç»¼åˆé¡¹ç›® (20%)

> âš ï¸ æ³¨æ„ï¼šä½œä¸šè¿‡ç¨‹å¯èƒ½å¯¹GPUèµ„æºæœ‰ä¸€å®šè¦æ±‚ï¼ˆæ„Ÿè°¢AMDçš„é¼åŠ›æ”¯æŒï¼‰

## ğŸš€ æŠ€æœ¯æ ˆ

### æ ¸å¿ƒæ¡†æ¶
- **[PyTorch](https://pytorch.org/)** - æ·±åº¦å­¦ä¹ æ¡†æ¶
- **[Transformers](https://github.com/huggingface/transformers)** - Hugging Faceçš„Transformeråº“
- **[PEFT](https://github.com/huggingface/peft)** - å‚æ•°é«˜æ•ˆå¾®è°ƒ
- **[Tokenizers](https://github.com/huggingface/tokenizers)** - æ–‡æœ¬åˆ†è¯å·¥å…·

### é«˜çº§æŠ€æœ¯
- **FlashAttention** - [é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶](https://github.com/Dao-AILab/flash-attention)
- **DeepSpeed** - [åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶](https://github.com/microsoft/DeepSpeed)
- **Megatron-LM** - [å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒ](https://github.com/NVIDIA/Megatron-LM)
- **Triton** - [GPUå†…æ ¸ä¼˜åŒ–](https://github.com/openai/triton)

## ğŸ“š å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»
- **Attention Is All You Need** - [Transformeræ¶æ„è®ºæ–‡](https://arxiv.org/abs/1706.03762)
- **LLaMA: Open and Efficient Foundation Language Models** - [LLaMAæ¨¡å‹è®ºæ–‡](https://arxiv.org/abs/2302.13971)
- **FlashAttention: Fast and Memory-Efficient Exact Attention** - [FlashAttentionè®ºæ–‡](https://arxiv.org/abs/2205.14135)

### å¼€æºé¡¹ç›®
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)

## ğŸ“ è”ç³»æ–¹å¼

- **æŠ€æœ¯é—®é¢˜**ï¼šåœ¨GitHub Issuesä¸­æé—®
- **ä¸ªäººå’¨è¯¢**ï¼šå‘é€é‚®ä»¶è‡³ jingweix@nju.edu.cn

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªè¯¾ç¨‹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒä¸€ä¸‹ï¼â­**

*è®©æ›´å¤šäººèƒ½å¤Ÿå­¦ä¹ åˆ°å¤§è¯­è¨€æ¨¡å‹çš„å¥¥ç§˜*

</div>
