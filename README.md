# 大语言模型基础：从零到一实现之路

[![Course Badge](https://img.shields.io/badge/Course-LLM%20Basics-blue.svg)](https://njudeepengine.github.io/llm-course-lecture/)
[![Year](https://img.shields.io/badge/Year-2024%2F2025-green.svg)]()
[![Language](https://img.shields.io/badge/Language-Chinese%2FEnglish-yellow.svg)]()

> 一门理论与实践并重的大语言模型课程，带你从零开始理解和实现LLM的各个组件（内容由Cursor生成hhh）

## 📚 课程简介

本课程旨在深入浅出地讲解大语言模型（Large Language Models, LLMs）的基础理论和实践实现。通过"从零到一"的学习路径，你将不仅理解LLM的工作原理，还能亲手实现各个核心组件。

### 🎯 课程目标

- **理论基础**：掌握LLM的数学原理和架构设计
- **实践能力**：从零开始实现LLM的各个组件
- **工程技能**：学习PyTorch、Transformers等主流框架的使用
- **前沿技术**：了解FlashAttention、分布式训练等高级技术

## 👨‍🏫 课程团队

- **主讲教师**：徐经纬
- **办公室**：计算机学院1022
- **邮箱**：jingweix@nju.edu.cn
- **课程联合创始人**：黄云鹏
- **2025助教团队**：赵世驹、梁明宇、卜韬、王乾刚、徐鼎坤

## 🌐 相关链接

- **课程主页**：[https://njudeepengine.github.io/llm-course-lecture/](https://njudeepengine.github.io/llm-course-lecture/)
- **作业主页**：[https://njudeepengine.github.io/LLM-Blog/](https://njudeepengine.github.io/LLM-Blog/)
- **B站课程视频**： [第1讲](https://www.bilibili.com/video/BV12RemzSEfV)、[第3讲](https://www.bilibili.com/video/BV1EJH9zRE2p)
- **QQ群**：1033682290 (NJU内部)

## 📖 课程大纲

### 2025年课程内容

1. **课程简介** - 课程概述和学习路径  
   讲义链接: https://njudeepengine.github.io/llm-course-lecture/2025/lecture1.html

   B站链接: https://www.bilibili.com/video/BV12RemzSEfV
2. **特征空间的变换1** - 前反向运行视角理解深度学习模型  
   讲义链接: https://njudeepengine.github.io/llm-course-lecture/2025/lecture2.html
3. **特征空间的变换2** - 前反向运行视角理解深度学习模型  
   讲义链接: https://njudeepengine.github.io/llm-course-lecture/2025/lecture3.html
   
   B站链接: https://www.bilibili.com/video/BV1EJH9zRE2p


### 2024年课程内容

#### 基础理论
1. **课程简介** - 课程概述和学习路径
2. **深度学习基础 I** - PyTorch基础和基本概念
3. **深度学习基础 II** - 反向传播和优化算法
4. **大语言模型解析 I** - Tokenizer和Positional Embedding
5. **大语言模型解析 II** - RMSNorm和MLP
6. **大语言模型解析 III** - Attention Layer
7. **大语言模型解析 IV** - Flash Attention
8. **大语言模型解析 V** - MoE和LoRA
9. **大语言模型解析 VI** - Fine-tuning

#### 高级技术
10. **大语言模型解析 VII** - LLM save/load
11. **大语言模型解析 VIII** - LLM推理、解码策略、KVCache
12. **大语言模型推理技术** - RAG基础
13. **大语言模型推理技术** - RAG进阶
14. **大语言模型进阶** - Megatron中的并行化技术


## 🎓 评分体系

- **从零到一之路**：5次编程作业 (80%)
- **启动：大作业**：综合项目 (20%)

> ⚠️ 注意：作业过程可能对GPU资源有一定要求（感谢AMD的鼎力支持）

## 🚀 技术栈

### 核心框架
- **[PyTorch](https://pytorch.org/)** - 深度学习框架
- **[Transformers](https://github.com/huggingface/transformers)** - Hugging Face的Transformer库
- **[PEFT](https://github.com/huggingface/peft)** - 参数高效微调
- **[Tokenizers](https://github.com/huggingface/tokenizers)** - 文本分词工具

### 高级技术
- **FlashAttention** - [高效注意力机制](https://github.com/Dao-AILab/flash-attention)
- **DeepSpeed** - [分布式训练框架](https://github.com/microsoft/DeepSpeed)
- **Megatron-LM** - [大规模语言模型训练](https://github.com/NVIDIA/Megatron-LM)
- **Triton** - [GPU内核优化](https://github.com/openai/triton)

## 📚 学习资源

### 推荐阅读
- **Attention Is All You Need** - [Transformer架构论文](https://arxiv.org/abs/1706.03762)
- **LLaMA: Open and Efficient Foundation Language Models** - [LLaMA模型论文](https://arxiv.org/abs/2302.13971)
- **FlashAttention: Fast and Memory-Efficient Exact Attention** - [FlashAttention论文](https://arxiv.org/abs/2205.14135)

### 开源项目
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)

## 📞 联系方式

- **技术问题**：在GitHub Issues中提问
- **个人咨询**：发送邮件至 jingweix@nju.edu.cn

---

<div align="center">

**⭐ 如果这个课程对你有帮助，请给个Star支持一下！⭐**

*让更多人能够学习到大语言模型的奥秘*

</div>
